#!/usr/bin/env python3
"""BiDirectional Sync using rclone"""

__version__ = "V3.2 201201"                         # Version number and date code

#==========================================================================================================
# Configure rclone, including authentication before using this tool.  rclone must be in the search path.
#
# Chris Nelson, November 2017 - 2020
# Contributions:
#   Hildo G. Jr., e2t, kalemas, and silenceleaf
#
# See README.md for revision history
#
# Known bugs:
#   Many print statements use .format vs. Py3 f strings, only for historical reasons.
#
#==========================================================================================================

import argparse
import sys
import re
import os.path
import io
import platform
import shutil
import subprocess
from datetime import timedelta
from datetime import datetime
import tempfile
import time
import logging
import inspect                                      # For getting the line number for error messages.
import collections                                  # For dictionary sorting.
import hashlib                                      # For checking if the filter file changed and force --first_sync.
import signal                                       # For keyboard interrupt handler

# pip install google-api-python-client
# pip install google_auth_oauthlib
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

import configparser
import json

# Configurations and constants
RCLONE_MIN_VERSION = 1.53
is_Windows = False
is_Linux = False
if sys.platform == "win32":
    is_Windows = True
if sys.platform == "linux":
    is_Linux = True

MAX_DELETE = 50                                     # % deleted allowed, else abort.  Use --force or --max_deletes to override.
CHK_FILE = 'RCLONE_TEST'

RTN_ABORT = 1                                       # Tokens for return codes based on criticality.
RTN_CRITICAL = 2                                    # Aborts allow rerunning.  Criticals block further runs.  See Readme.md.

gdrive="../gdrive/gdrive"
path1_incremental=True
skip_refresh=True # we should avoid the final refresh in case someone adds/removes a file between when we sync and when we refresh

def add_suffix_to_fname(fname,suffix):
    dotpos=fname.rfind(".")
    if dotpos!=-1 and dotpos<(len(fname)-1):
        new_fname=fname[:dotpos]+suffix+fname[dotpos:]
    else:
        new_fname=fname + suffix
    return(new_fname)

def bidirSync():

    global path1_lsl_file, path2_lsl_file
    global path1_lsl_token # TODO
    global lsl_temp_filter
    lsl_file_base  = workdir + "LSL_" + (path1_base + path2_base).replace(':','_').replace(r'/','_').replace('\\','_')
            # eg:  '/home/<user>/.rclonesyncwd/LSL_<path1_base><path2_base>'
    path1_lsl_file = lsl_file_base + '_Path1'
    path1_lsl_token = lsl_file_base + '_Path1_token'
    path2_lsl_file = lsl_file_base + '_Path2'
    lsl_temp_filter = lsl_file_base + '_temp_filter'

    
    # ***** Check Sync Only *****
    def check_sync():           # Used here and at the end of the flow
        _, path1_contents = load_list(path1_lsl_file)
        _, path2_contents = load_list(path2_lsl_file)

        sync_integrity_fail = False
        for key in path1_contents:
            if key not in path2_contents:
                logging.info(print_msg("ERROR", "Path1 file not found in Path2", key))
                sync_integrity_fail = True
        for key in path2_contents:
            if key not in path1_contents:
                logging.info(print_msg("ERROR", "Path2 file not found in Path1", key))
                sync_integrity_fail = True
        if sync_integrity_fail:
            logging.error("ERROR: The content of Path1 and Path2 are out of sync.  --first-sync required to recover.")
            return 1
        return 0
        
        path1_contents = None
        path2_contents = None

    if args.check_sync_only:
        logging.info(f">>>>> Checking integrity of LSL history files for Path1  <{path1_base}>  versus Path2  <{path2_base}>")
        if check_sync():
            return RTN_CRITICAL
        return 0
    

    logging.info(f"Synching Path1  <{path1_base}>  with Path2  <{path2_base}>")

    args_string = ''
    for arg in sorted(args.__dict__):
        argvalue = getattr(args, arg)
        if type(argvalue) is int:
            argvalue = str(argvalue)
        if type(argvalue) is bool:
            if argvalue is False:
                argvalue = "False"
            else:
                argvalue = "True"
        if type(argvalue) is list:              # --rclone-args case
            rcargs = '=['
            for item in argvalue:
                rcargs += item + ' '
            argvalue = rcargs[:-1] + ']'
        if argvalue is None:
            argvalue = "None"
        args_string += arg + '=' + argvalue + ', '
    logging.info ("Command args: <{}>".format(args_string[:-2]))


    # ***** Handle user_filter_file, if provided *****
    if user_filter_file is not None:
        logging.info("Using filters-file  <{}>".format(user_filter_file))

        if not os.path.exists(user_filter_file):
            logging.error("Specified filters-file file does not exist:  " + user_filter_file)
            return RTN_CRITICAL

        user_filter_file_MD5 = user_filter_file + "-MD5"

        with io.open(user_filter_file, 'rb') as ifile:
            current_file_hash = bytes(hashlib.md5(ifile.read()).hexdigest(), encoding='utf-8')

        stored_file_hash = ''
        if os.path.exists(user_filter_file_MD5):
            with io.open(user_filter_file_MD5, mode="rb") as ifile:
                stored_file_hash = ifile.read()
        elif not first_sync:
            logging.error("MD5 file not found for filters file <{}>.  Must run --first-sync.".format(user_filter_file))
            return RTN_CRITICAL

        if current_file_hash != stored_file_hash and not first_sync:
            logging.error("Filters-file <{}> has chanaged (MD5 does not match).  Must run --first-sync.".format(user_filter_file))
            return RTN_CRITICAL

        if first_sync:
            logging.info("Storing filters-file hash to <{}>".format(user_filter_file_MD5))
            with io.open(user_filter_file_MD5, 'wb') as ofile:
                ofile.write(current_file_hash)


    # ***** Set up --dry-run, and rclone --verbose and --log-format switches *****
    switches = []
    for _ in range(rc_verbose):
        switches.append("-v")
    if dry_run:     # If dry_run, original LSL files are preserved and lsl's are done to the _DRYRUN files.
        switches.append("--dry-run")
        if os.path.exists(path1_lsl_file):
            shutil.copy(path1_lsl_file, path1_lsl_file + '_DRYRUN')
        path1_lsl_file += '_DRYRUN'
        if os.path.exists(path1_lsl_token):
            shutil.copy(path1_lsl_token, path1_lsl_token + '_DRYRUN')
        path1_lsl_token += '_DRYRUN'
        if os.path.exists(path2_lsl_file):          
            shutil.copy(path2_lsl_file, path2_lsl_file + '_DRYRUN')
        path2_lsl_file  += '_DRYRUN'
    if args.no_datetime_log:
        switches.extend(['--log-format', '""'])


    # ***** first_sync generate path1 and path2 file lists, and copy any unique path2 files to path1 ***** 
    if first_sync:
        logging.info(">>>>> --first-sync copying any unique Path2 files to Path1")

        if path1_incremental:
            path1_lsl_token_new = path1_lsl_token + '_NEW'
            token_fp=open(path1_lsl_token_new,"wt")
            token_fp.write(gdrive_changes_now(path1_base)+"\n")
            token_fp.close()

        path1_lsl_file_new = path1_lsl_file + '_NEW'
        status, path1_now = get_and_load_lsl("current Path1", path1_lsl_file_new, path1_base)
        if status:  return status

        path2_lsl_file_new = path2_lsl_file + '_NEW'
        status, path2_now = get_and_load_lsl("current Path2", path2_lsl_file_new, path2_base)
        if status:  return status

        files_first_sync_copy_P2P1 = []
        for key in path2_now:
            if key not in path1_now:
                logging.info(print_msg("Path2", "  --first-sync queue copy to Path1", key))
                files_first_sync_copy_P2P1.append(key)

        if len(files_first_sync_copy_P2P1) > 0:
            files_first_sync_copy_P2P1_filename = lsl_file_base + "_files_first_sync_copy_P2P1"
            with io.open(files_first_sync_copy_P2P1_filename, mode='wt', encoding='utf8') as outf:
                for item in files_first_sync_copy_P2P1:
                    outf.write(item + "\n")
            logging.info(print_msg("Path2", "  Do queued first-sync copies to", "Path1"))
            if rclone_cmd('copy', path2_base, path1_base, files_file=files_first_sync_copy_P2P1_filename, options=switches):
                return RTN_CRITICAL
            if not args.no_cleanup:
                os.remove(files_first_sync_copy_P2P1_filename)

        
        logging.info(">>>>> --first-sync synching Path1 to Path2")
        # NOTE:  --min-size 0 added to block attempting to overwrite Google Doc files which have size -1 on Google Drive.  180729
        if rclone_cmd('sync', path1_base, path2_base, filter_file=user_filter_file, options=switches + ['--min-size', '0']):
            return RTN_CRITICAL

        if skip_refresh:
            logging.info(">>>>> --first-sync skipping refreshing lsl files")
            shutil.copy2(path1_lsl_file_new, path1_lsl_file)
            shutil.copy2(path2_lsl_file_new, path2_lsl_file)
        else:
            logging.info(">>>>> --first-sync refreshing lsl files")
            if rclone_lsl(path1_base, path1_lsl_file, filter_file=user_filter_file):
                return RTN_CRITICAL

            if rclone_lsl(path2_base, path2_lsl_file, filter_file=user_filter_file):
                return RTN_CRITICAL

        # success. We can finalize the change token
        if path1_incremental:
            shutil.copy2(path1_lsl_token_new, path1_lsl_token)

        if not args.no_cleanup:
            os.remove(path1_lsl_file_new)
            os.remove(path2_lsl_file_new)
            if path1_incremental:
                os.remove(path1_lsl_token_new)

        return 0
    

    # ***** Check for existence of prior Path1 and Path2 lsl files *****
    if (not os.path.exists(path1_lsl_file) or not os.path.exists(path2_lsl_file)):
        # On prior critical error abort, the prior LSL files are renamed to _ERROR to lock out further runs
        logging.error("***** Cannot find prior Path1 or Path2 lsl files, likely due to critical error on prior run.")
        return RTN_CRITICAL


    # ***** Check for Path1 deltas relative to the prior sync *****
    logging.info(">>>>> Path1 Checking for Diffs")

    if path1_incremental:
        token_fp=open(path1_lsl_token,"rt")
        change_token=token_fp.readline().rstrip()
        token_fp.close()

    logging.info("[MB] PROBLEM: ASSUMPTION: PATH1 IS THE REMOTE GOOGLE DRIVE. PATH2 IS THE LOCAL PATH") # TODO


    path1_prior = None
    path1_prior_count = None
    path1_may_have_changed=True
    path1_lsl_file_new = path1_lsl_file + '_NEW'
    if path1_incremental:
        status, path1_now, path1_prior, path1_may_have_changed, change_token = incremental_get_and_load_lsl("current Path1", change_token, path1_lsl_file, path1_lsl_file_new, path1_base)
        logging.debug("**************SAME!!") if path1_now == path1_prior else logging.debug("***************DIFF!!!")
        logging.debug("**************path1_may_have_changed="+str(path1_may_have_changed))
    else:
        status, path1_now = get_and_load_lsl("current Path1", path1_lsl_file_new, path1_base)
    if status:  return status


    def get_check_files (loaded_lsl):
        check_files = {}
        for key in loaded_lsl:
            if args.check_filename in key and "rclonesync/Test/" not in key:
                check_files[key] = loaded_lsl[key]
        return check_files

    if check_access and path1_may_have_changed:
        path1_check = get_check_files(path1_now)

    if path1_prior == None and path1_may_have_changed:
        logging.debug("Loading Path1 list")
        status, path1_prior = get_and_load_lsl("prior Path1", path1_lsl_file)
        if status:  return status
    if path1_prior != None:
        path1_prior_count = len(path1_prior)    # Save for later max deletes check

    path1_deltas = {}
    path1_deleted = 0
    path1_found_same = True
    if path1_may_have_changed:
        logging.debug("looking for changes in Path1")
        path1_found_same = False
        for key in path1_prior:
            _newer=False; _older=False; _size=False; _deleted=False
            if key not in path1_now:
                logging.info(print_msg("Path1", "  File was deleted", key))
                path1_deleted += 1
                _deleted = True
            else:
                if path1_prior[key]['datetime'] != path1_now[key]['datetime']:
                    if path1_prior[key]['datetime'] < path1_now[key]['datetime']:
                        logging.info(print_msg("Path1", "  File is newer", key))
                        _newer = True
                    else:               # Current path1 version is older than prior sync.
                        logging.info(print_msg("Path1", "  File is OLDER", key))
                        _older = True
    
            if _newer or _older or _size or _deleted:
                path1_deltas[key] = {'new':False, 'newer':_newer, 'older':_older, 'size':_size, 'deleted':_deleted}
            else:
                path1_found_same = True # Once we've found at least 1 unchanged file we know that not everything has changed, as with a DST time change

        logging.debug("looking for new files in Path1")
        logging.debug("==============")
        for key in path1_now:
            if key.endswith("my ADDRESS.txt"):
                logging.debug("present in path1_now: "+key)
        for key in path1_prior:
            if key.endswith("my ADDRESS.txt"):
                logging.debug("present in path1_prior: "+key)
        logging.debug("==============")
        for key in path1_now:
            if key not in path1_prior:
                logging.info(print_msg("Path1", "  File is new", key))
                path1_deltas[key] = {'new':True, 'newer':False, 'older':False, 'size':False, 'deleted':False}
        logging.debug("done looking for new files in Path1")

    path1_prior = None              # Free up the memory
#    path1_now = None

    path1_deltas = collections.OrderedDict(sorted(path1_deltas.items()))    # Sort the deltas list.
    if len(path1_deltas) > 0:
        news = newers = olders = deletes = 0
        for key in path1_deltas:
            if path1_deltas[key]['new']:      news += 1
            if path1_deltas[key]['newer']:    newers += 1
            if path1_deltas[key]['older']:    olders += 1
            if path1_deltas[key]['deleted']:  deletes += 1
        logging.info(f"  {len(path1_deltas):4} file change(s) on Path1: {news:4} new, {newers:4} newer, {olders:4} older, {deletes:4} deleted")


    # ***** Check for Path2 deltas relative to the prior sync *****
    logging.info(">>>>> Path2 Checking for Diffs")

    path2_lsl_file_new = path2_lsl_file + '_NEW'
    status, path2_now = get_and_load_lsl("current Path2", path2_lsl_file_new, path2_base)
    if status:  return status

    if check_access:
        path2_check = get_check_files(path2_now)

    status, path2_prior = get_and_load_lsl("prior Path2", path2_lsl_file)
    if status:  return status
    path2_prior_count = len(path2_prior)    # Save for later max deletes check

    path2_deltas = {}
    path2_deleted = 0
    path2_found_same = False
    for key in path2_prior:
        _newer=False; _older=False; _size=False; _deleted=False
        if key not in path2_now:
            logging.info(print_msg("Path2", "  File was deleted", key))
            logging.debug(key)
            path2_deleted += 1
            _deleted = True
        else:
            if path2_prior[key]['datetime'] != path2_now[key]['datetime']:
                if path2_prior[key]['datetime'] < path2_now[key]['datetime']:
                    logging.info(print_msg("Path2", "  File is newer",key+" "+str(path2_prior[key]['datetime'])+" "+str(path2_now[key]['datetime'])))
                    _newer = True
                else:               # Current Path2 version is older than prior sync.
                    logging.info(print_msg("Path2", "  File is OLDER", key))
                    _older = True

        if _newer or _older or _size or _deleted:
            path2_deltas[key] = {'new':False, 'newer':_newer, 'older':_older, 'size':_size, 'deleted':_deleted}
        else:
            path2_found_same = True # Once we've found at least 1 unchanged file we know that not everything has changed, as with a DST time change

    for key in path2_now:
        if key not in path2_prior:
            logging.info(print_msg("Path2", "  File is new", key))
            path2_deltas[key] = {'new':True, 'newer':False, 'older':False, 'size':False, 'deleted':False}

    path2_prior = None              # Free up the memory
#    path2_now = None

    path2_deltas = collections.OrderedDict(sorted(path2_deltas.items()))      # Sort the deltas list.
    if len(path2_deltas) > 0:
        news = newers = olders = deletes = 0
        for key in path2_deltas:
            if path2_deltas[key]['new']:      news += 1
            if path2_deltas[key]['newer']:    newers += 1
            if path2_deltas[key]['older']:    olders += 1
            if path2_deltas[key]['deleted']:  deletes += 1
        logging.info(f"  {len(path2_deltas):4} file change(s) on Path2: {news:4} new, {newers:4} newer, {olders:4} older, {deletes:4} deleted")


    # ***** Check access health to the Path1 and Path2 filesystems *****
    if check_access:
        logging.info(">>>>> Checking Path1 and Path2 rclone filesystems access health")

        check_error = False
        if len(path1_check) < 1 or len(path1_check) != len(path2_check):
            logging.error(print_msg("ERROR", "Failed access health test:  <{}> Path1 count {}, Path2 count {}"
                                        .format(chk_file, len(path1_check), len(path2_check)), ""))
            check_error = True

        for key in path1_check:
            if key not in path2_check:
                logging.error(print_msg("ERROR", "Failed access health test:  Path1 key <{}> not found in Path2".format(key), ""))
                check_error = True
        for key in path2_check:
            if key not in path1_check:
                logging.error(print_msg("ERROR", "Failed access health test:  Path2 key <{}> not found in Path1".format(key), ""))
                check_error = True

        if check_error:
            return RTN_CRITICAL

        logging.info(f"  Found <{len(path1_check)}> matching <{args.check_filename}> files on both paths")


    # ***** Check for too many deleted files - possible error condition and don't want to start deleting on the other side !!! *****
    too_many_path1_deletes = False
    if not force and path1_prior_count != None and float(path1_deleted)/path1_prior_count > float(max_deletes)/100:
        logging.error("SAFETY ABORT - Excessive number of deletes (>{}%, {} of {}) found on the Path1 filesystem <{}>.  Run with --force if desired."
                       .format(max_deletes, path1_deleted, path1_prior_count, path1_base))
        too_many_path1_deletes = True

    too_many_path2_deletes = False
    if not force and float(path2_deleted)/path2_prior_count > float(max_deletes)/100:
        logging.error("SAFETY ABORT - Excessive number of deletes (>{}%, {} of {}) found on the Path2 filesystem <{}>.  Run with --force if desired."
                       .format(max_deletes, path2_deleted, path2_prior_count, path2_base))
        too_many_path2_deletes = True

    if too_many_path1_deletes or too_many_path2_deletes:
        return RTN_ABORT


    # ***** Check for all files changed, such as all dates changed due to DST change, to avoid errant copy everything.  See README.md. *****
    if not force and not path1_found_same:
        logging.error(f"SAFETY ABORT - All files were found to be changed on the Path1 filesystem <{path1_base}>.  Something is possibly wrong.  Run with --force if desired.")
        return RTN_ABORT
        
    if not force and not path2_found_same:
        logging.error(f"SAFETY ABORT - All files were found to be changed on the Path2 filesystem <{path2_base}>.  Something is possibly wrong.  Run with --force if desired.")
        return RTN_ABORT
        

    # ***** Determine and apply changes to Path1 and Path2 *****
    files_copy_P1P2 = []
    files_copy_P2P1 = []
    files_delete_P1 = []
    files_delete_P2 = []
    already_handled = {}

    if len(path1_deltas) == 0 and len(path2_deltas) == 0:
        logging.info(">>>>> No changes on Path1 or Path2")
    else:
        logging.info(">>>>> Determining and applying changes")

        for key in path1_deltas:
            if path1_deltas[key]['new'] or path1_deltas[key]['newer'] or path1_deltas[key]['older']:
                if key not in path2_deltas:
                    logging.info(print_msg("Path1", "  Queue copy to Path2", path2_base + key))
                    files_copy_P1P2.append(key)

                elif path2_deltas[key]['deleted']:
                    logging.info(print_msg("Path1", "  Queue copy to Path2", path2_base + key))
                    files_copy_P1P2.append(key)
                    already_handled[key] = 1

                elif path2_deltas[key]['new'] or path2_deltas[key]['newer'] or path2_deltas[key]['older']:
                    new_key=add_suffix_to_fname(key,"_Path1")
                    logging.warning(print_msg("WARNING", "  New or changed in both paths", key))
                    logging.warning(print_msg("Path1", "  Renaming Path1 copy", path1_base + new_key))

                    if path1_now == None:
                        status, path1_now = load_list(path1_lsl_file)
                        if status:
                            logging.error(print_msg("ERROR", f"Failed loading {path_text} list file <{lsl_file}>"))
                            return RTN_ABORT

                    if rclone_cmd('moveto', path1_base + key, path1_base + new_key, options=switches):
                        return RTN_CRITICAL
                    path1_now[new_key]=path1_now[key]
                    path1_now[new_key]['moved-from']=key
                    path1_now[key]['moved-to']=new_key

                    logging.warning(print_msg("Path1", "  Queue copy to Path2", path2_base + new_key))
                    files_copy_P1P2.append(new_key)

                    new_key=add_suffix_to_fname(key,"_Path2")
                    logging.warning(print_msg("Path2", "  Renaming Path2 copy", path2_base + new_key))
                    if rclone_cmd('moveto', path2_base + key, path2_base + new_key, options=switches):
                        return RTN_CRITICAL
                    path2_now[new_key]=path2_now[key]
                    path2_now[new_key]['moved-from']=key
                    path2_now[key]['moved-to']=new_key
                    logging.warning(print_msg("Path2", "  Queue copy to Path1", path1_base + new_key))
                    files_copy_P2P1.append(new_key)
                    already_handled[key] = 1

            else: # Path1 deleted
                if key not in path2_deltas:
                    logging.info(print_msg("Path2", "  Queue delete", path2_base + key))
                    files_delete_P2.append(key)
                elif path2_deltas[key]['new'] or path2_deltas[key]['newer'] or path2_deltas[key]['older']:
                    logging.info(print_msg("Path2", "  Queue copy to Path1", path1_base + key))
                    files_copy_P2P1.append(key)
                    already_handled[key] = 1
                elif path2_deltas[key]['deleted']:
                    already_handled[key] = 1

        for key in path2_deltas:
            logging.info(print_msg("Path2", "  processing file: ", key))
            if key not in already_handled:
                if path2_deltas[key]['new'] or path2_deltas[key]['newer'] or path2_deltas[key]['older']:
                    logging.info(print_msg("Path2", "  Queue copy to Path1", path1_base + key))
                    files_copy_P2P1.append(key)
                else: # Deleted 
                    logging.info(print_msg("Path1", "  Queue delete", path1_base + key))
                    files_delete_P1.append(key)

        path1_changes = False
        path2_changes = False
        # Do the batch operation
        if len(files_copy_P2P1) > 0:
            path1_changes = True
            files_copy_P2P1_filename = lsl_file_base + "_files_copy_P2P1"
            with io.open(files_copy_P2P1_filename, mode='wt', encoding='utf8') as outf:
                for item in files_copy_P2P1:
                    outf.write(item + "\n")
            logging.info(print_msg("Path2", "  Do queued copies to", "Path1"))
            if rclone_cmd('copy', path2_base, path1_base, files_file=files_copy_P2P1_filename, options=switches):
                return RTN_CRITICAL
            if not args.no_cleanup:
                os.remove(files_copy_P2P1_filename)

        if len(files_copy_P1P2) > 0:
            path2_changes = True
            files_copy_P1P2_filename = lsl_file_base + "_files_copy_P1P2"
            with io.open(files_copy_P1P2_filename, mode='wt', encoding='utf8') as outf:
                for item in files_copy_P1P2:
                    outf.write(item + "\n")
            logging.info(print_msg("Path1", "  Do queued copies to", "Path2"))
            if rclone_cmd('copy', path1_base, path2_base, files_file=files_copy_P1P2_filename, options=switches):
                return RTN_CRITICAL
            if not args.no_cleanup:
                os.remove(files_copy_P1P2_filename)

        if len(files_delete_P1) > 0:
            path1_changes = True
            files_delete_P1_filename = lsl_file_base + "_files_delete_P1"
            with io.open(files_delete_P1_filename, mode='wt', encoding='utf8') as outf:
                for item in files_delete_P1:
                    outf.write(item + "\n")
            logging.info(print_msg("", "  Do queued deletes on", "Path1"))
            if rclone_cmd('delete', path1_base, files_file=files_delete_P1_filename, options=switches):
                return RTN_CRITICAL
            if not args.no_cleanup:
                os.remove(files_delete_P1_filename)

        if len(files_delete_P2) > 0:
            path2_changes = True
            files_delete_P2_filename = lsl_file_base + "_files_delete_P2"
            with io.open(files_delete_P2_filename, mode='wt', encoding='utf8') as outf:
                for item in files_delete_P2:
                    outf.write(item + "\n")
            logging.info(print_msg("", "  Do queued deletes on", "Path2"))
            if rclone_cmd('delete', path2_base, files_file=files_delete_P2_filename, options=switches):
                return RTN_CRITICAL
            if not args.no_cleanup:
                os.remove(files_delete_P2_filename)

#        files_copy_P1P2 = None      # Free up the memory
#        files_copy_P2P1 = None
#        files_delete_P1 = None
#        files_delete_P2 = None
#        already_handled = None


    # ***** Clean up and check LSL files integrity *****
    if skip_refresh:
        logging.info(">>>>> Skipping refresh of Path1 and Path2 lsl files")
#        if path1_may_have_changed: shutil.copy2(path1_lsl_file_new, path1_lsl_file) # TODO: redo this
        if len(files_copy_P2P1)==0 and len(files_delete_P1)==0:
            if path1_now != None and path1_may_have_changed: # otherwise, there is no new file and no changes
                shutil.copy2(path1_lsl_file_new, path1_lsl_file)
        else:
            if path1_now == None:
                status, path1_now = load_list(path1_lsl_file)
                if status:
                    logging.error(print_msg("ERROR", f"Failed loading {path_text} list file <{lsl_file}>"))
                    return RTN_ABORT
            for key in files_copy_P2P1:
                if 'moved-from' in path2_now[key]:
                    del path2_now[path2_now[key]['moved-from']]
                    del path2_now[key]['moved-from']
                logging.info("adding to Path1 cache file "+key+"; data="+str(path2_now[key]))
                path1_now[key]=path2_now[key]
            for key in files_delete_P1:
                logging.info("deleting from Path1 cache file "+key)
                if key in path1_now: del path1_now[key]
            order_and_write_lsl(path1_now,path1_lsl_file)

        if len(files_copy_P1P2)==0 and len(files_delete_P2)==0:
            shutil.copy2(path2_lsl_file_new, path2_lsl_file)
        else:
            if path1_now == None:
                status, path1_now = load_list(path1_lsl_file)
                if status:
                    logging.error(print_msg("ERROR", f"Failed loading {path_text} list file <{lsl_file}>"))
                    return RTN_ABORT
            for key in files_copy_P1P2:
                if 'moved-from' in path1_now[key]:
                    del path1_now[path1_now[key]['moved-from']]
                    del path1_now[key]['moved-from']
                logging.info("adding to Path2 cache file "+key+"; data="+str(path1_now[key]))
                path2_now[key]=path1_now[key]
            for key in files_delete_P2:
                logging.info("deleting from Path2 cache file "+key)
                if key in path2_now: del path2_now[key]
            order_and_write_lsl(path2_now,path2_lsl_file)
    else:
        logging.info(">>>>> Refreshing Path1 and Path2 lsl files")
        if len(path1_deltas) == 0 and len(path2_deltas) == 0:
            if path1_may_have_changed: shutil.copy2(path1_lsl_file_new, path1_lsl_file)
            shutil.copy2(path2_lsl_file_new, path2_lsl_file)
        else:
            if path1_changes and not path1_incremental:
                if rclone_lsl(path1_base, path1_lsl_file, filter_file=user_filter_file):
                    return RTN_CRITICAL
            else:
                if path1_may_have_changed: shutil.copy2(path1_lsl_file_new, path1_lsl_file)

            if path2_changes:
                if rclone_lsl(path2_base, path2_lsl_file, filter_file=user_filter_file):
                    return RTN_CRITICAL
            else:
                shutil.copy2(path2_lsl_file_new, path2_lsl_file)

    path1_now = None              # Free up the memory
    path2_now = None

    files_copy_P1P2 = None      # Free up the memory
    files_copy_P2P1 = None
    files_delete_P1 = None
    files_delete_P2 = None
    already_handled = None

    if path1_incremental:
        # Success. We can update the change token
        token_fp=open(path1_lsl_token,"wt")
        token_fp.write(change_token+"\n")
        token_fp.close()

    if not args.no_cleanup:
        if path1_may_have_changed: os.remove(path1_lsl_file_new)
        os.remove(path2_lsl_file_new)

    path1_deltas    = None      # Free up the memory
    path2_deltas    = None

    if not args.no_check_sync and not dry_run:
        logging.info(f">>>>> Checking integrity of LSL history files for Path1  <{path1_base}>  versus Path2  <{path2_base}>")
        if check_sync():
            return RTN_CRITICAL


    # ***** Optional rmdirs for empty directories *****
    if rmdirs:
        logging.info(">>>>> rmdirs Path1")
        if rclone_cmd('rmdirs', path1_base, filter_file=user_filter_file, options=switches):
            return RTN_CRITICAL

        logging.info(">>>>> rmdirs Path2")
        if rclone_cmd('rmdirs', path2_base, filter_file=user_filter_file, options=switches):
            return RTN_CRITICAL

    return 0


# =====  Support functions  ==========================================================
def print_msg(tag, msg, key=''):
    return "  {:9}{:35} - {}".format(tag, msg, key)


# ***** rclone call wrapper functions with retries *****
MAXTRIES=3
def rclone_lsl(path, ofile, filter_file=None, options=None, extra_filter_file=None, max_depth=-1):
    """
    Fetch an rclone LSL of the path and write it to ofile.
    filter_file is a string full path to a file which will be passed to rclone --filter-from.
    options is a list of switches passed to rclone (not currently used)
    """
    linenum = inspect.getframeinfo(inspect.stack()[1][0]).lineno
#    if individual_file_path=="":
    process_args = [rclone, "lsl", path, "--config", rcconfig]
#    else:
#        process_args = [rclone, "lsl", individual_file_path, "--config", rcconfig]
    if filter_file is not None: # and individual_file_path=="":
        process_args.extend(["--filter-from", filter_file])
    if extra_filter_file is not None:
        process_args.extend(["--filter-from", extra_filter_file])
    if max_depth != -1:
        process_args.extend(["--max-depth", str(max_depth)])
    if options is not None:
        process_args.extend(options)
    if args.rclone_args is not None:
        process_args.extend(args.rclone_args)
    logging.debug("    rclone command:  {}".format(process_args))
    for x in range(MAXTRIES):
        with io.open(ofile, "wt", encoding='utf8') as of:
            if not subprocess.call(process_args, stdout=of):
                return 0
            logging.info(print_msg("WARNING", "rclone lsl try {} failed.".format(x+1)))
    logging.error(print_msg("ERROR", "rclone lsl failed.  Specified path invalid?  (Line {})".format(linenum)))
    return 1


def gdrive_changes_now_gdrive(path):
    linenum = inspect.getframeinfo(inspect.stack()[1][0]).lineno
    logging.info("    PROBLEM: not passing a drive path to gdrive in gdrive_info(). Assuming that the configuration already exists.")

    process_args = [gdrive, "changes", "--now"]
    logging.debug("    rclone command:  {}".format(process_args))
    pout, perr = subprocess.Popen(process_args, stdout=subprocess.PIPE,universal_newlines=True).communicate()
    res=pout.splitlines()
    logging.debug("gdrive changes --now returned "+str(res))

    if len(res)<1: # some error
        logging.debug("    gdrive_changes_now() did not return anything!")
        return("")

    # drop the "Page token: " prefixes
    return(res[0].split(": ",1)[1])

aux_service={}

def googleapi_service(path):
    global aux_service

    key=path.split(":")[0]
    if key not in aux_service:
        creds = None
        # The file token.json stores the user's access and refresh tokens, and is
        # created automatically when the authorization flow completes for the first
        # time.
        if not os.path.exists(rcconfig):
            logging.error("PROBLEM: googleapi_service() cannot find rclone config file "+rcconfig)
            return(None)
        #creds = Credentials.from_authorized_user_file('token.json', SCOPES)

        c=configparser.ConfigParser()
        with open(rcconfig,'r') as fp:
            c.read_file(fp)
            if c[key]['type']!='drive':
                loging.error("ERROR: path "+path+" MUST be a Google Drive. Type found in rclone is "+c[key]['type'])
                return(None)
            tok=json.loads(c[key]['token'])
            data={}
            data['token']=tok['access_token']
            data['refresh_token']=tok['refresh_token']
            data['token_uri']="https://oauth2.googleapis.com/token"
            data['client_id']=c[key]['client_id']
            data['client_secret']=c[key]['client_secret']
#            data['scopes']=["https://www.googleapis.com/auth/drive"]
            data['expiry']=tok['expiry']

            logging.debug("***"+str(data))
            SCOPES = ['https://www.googleapis.com/auth/drive']
            creds = Credentials.from_authorized_user_info(data, SCOPES)
            try:
                # If there are no (valid) credentials available, let the user log in.
                if not creds or not creds.valid:
                    if creds and creds.expired and creds.refresh_token:
                        creds.refresh(Request())
                    else:
                        logging.error("PROBLEM: googleapi_service() cannot create credentials with the info in "+rcconfig)
                        return(None)
#                   else:
#                       flow = InstalledAppFlow.from_client_secrets_file(
#                           'credentials.json', SCOPES)
#                       creds = flow.run_local_server(port=0)

#                # Save the credentials for the next run
#                with open('token.json', 'w') as token:
#                    token.write(creds.to_json())

                aux_service[key] = build('drive', 'v3', credentials=creds)
            #except HttpError as error:
            except Exception as error:
                # TODO(developer) - Handle errors from drive API.
                logging.error(f'googleapi_service(): An error occurred: {error}')
                logging.error(f'googleapi_service(): likely unable to connect to the authentication server. Is your device connected to the internet?')
                return(None)

    return(aux_service[key])


def gdrive_changes_now_python(path):
    service=googleapi_service(path)
    if service==None:
        return("")

    response = service.changes().getStartPageToken().execute()
    return(response.get('startPageToken'))

def gdrive_changes_now(path):
    return(gdrive_changes_now_python(path))

def gdrive_info(path, file_id):
    linenum = inspect.getframeinfo(inspect.stack()[1][0]).lineno
    logging.info("    PROBLEM: not passing a drive path to gdrive in gdrive_info(). Assuming that the configuration already exists.")

    process_args = [gdrive, "info", file_id]
    logging.debug("    rclone command:  {}".format(process_args))
    pout, perr = subprocess.Popen(process_args, stdout=subprocess.PIPE,universal_newlines=True).communicate()
    res=pout.splitlines()
    logging.debug("gdrive info="+str(res))

    if len(res)<1 or res[0].startswith("Failed"): # some error, e.g. gdrive is unable to find the file info
        logging.debug("    gdrive_info(): gdrive info for "+file_id+" failed with error: "+str(res))
        return("")

    # drop the "line-type: " prefixes
    res=[l.split(": ",1)[1] for l in res]

    fid=res[0] # unused
    name=res[1] # unused
    path=res[2]
    mime=res[3]
    ctime=res[4] # unused
    mtime=res[5] # unused
    shared_flag=res[6] # unused
    viewurl=res[7] # unused

    logging.debug("TODO: support Google Apps files") # TODO
    logging.debug("mime="+mime) # TODO
    return(path if mime!="application/vnd.google-apps.folder" and not mime.startswith("application/vnd.google-apps.") else "")

def write_lsl(data,ofile):
    with io.open(ofile, "wt", encoding='utf8') as of:
        for p in data:
            of.write(data[p]['size']+" "+data[p]['orig_datetime']+" "+p+"\n")

def order_and_write_lsl(data,ofile):
    write_lsl(collections.OrderedDict(sorted(data.items())),ofile)

def merge_gdrive_changes_gdrive(token, path, lsl_file_cached, ofile, filter_file=None, options=None):
    """
    Fetch an rclone LSL of the path and write it to ofile.
    filter_file is a string full path to a file which will be passed to rclone --filter-from.
    options is a list of switches passed to rclone (not currently used)
    """
    global lsl_temp_filter

    orig_token=token
    linenum = inspect.getframeinfo(inspect.stack()[1][0]).lineno
    logging.info("    PROBLEM: not passing a drive path to gdrive in merge_gdrive_changes(). Assuming that the configuration already exists.")
    changed=[]
    keep_looping=True
    while keep_looping:
        process_args = [gdrive, "changes", "--since", str(token), "-m", "1000", "--no-header"]
        logging.debug("    gdrive command:  {}".format(process_args))
        pout, perr = subprocess.Popen(process_args, stdout=subprocess.PIPE,universal_newlines=True).communicate()
        logging.debug("    gdrive command completed")
        res=pout.splitlines()

        logging.debug("first line of gdrive changes: "+res[0])
        if res[0].startswith("Failed listing changes"):
            logging.error(print_msg("ERROR", "gdrive changes reported error:\n"+"\n".join(res)+"\n  (Line {})".format(linenum)))
            return 1, orig_token, True, None, None

        if res[0]=='No changes':
            return 0, orig_token, False, None, None
        
        token_line=res[-1]
        res=res[:-1] # remove token line
        for l in res:
            if l=="": # gdrive outputs an empty line at the end
                continue
            m=re.match(r"^([^ ]+) +(.*) ([a-z]+) +([0-9]{4}-[0-9]{2}-[0-9]{2}) +([0-9]{2}:[0-9]{2}:[0-9]{2})$",l)
            if m == None:
                logging.error(print_msg("ERROR", "gdrive changes returned an invalid line: '"+l+"'  (Line {})".format(linenum)))
                return 1, orig_token, True, None, None
            changed.append([m.group(i) for i in range(1,6)])
        m = re.match(r"Token: ([^,]*), more: (\w+)", token_line)
        if m == None:
            logging.error(print_msg("ERROR", "gdrive changes returned an invalid final line: '"+token_line+"'  (Line {})".format(linenum)))
            return 1, orig_token, True, None, None
        token=m.group(1)
        keep_looping=True if m.group(2)=="true" else False

    # load the cached file
    logging.debug("loading prior file list")
    status, cached_data = get_and_load_lsl(path, lsl_file_cached)
    logging.debug("done loading prior file list")
    if status != 0:
        return 1, orig_token, True, None, None

    remote_root=path.split(":",1)[1]
    if not remote_root.endswith("/"):
        remote_root=remote_root+"/"
    if remote_root=="/":
        remote_root=""
    changes_made=False
    new_data=cached_data.copy()
    tempfile=ofile+"_SINGLE"
    for entry in changed:
        if len(entry)==0:
            break # we are done
        i=0
        file_id=entry[i]
        i+=1
        file_name=entry[i] # not used
        i+=1
        file_action=entry[i]
        i+=1
        file_date=entry[i] # not used
        i+=1
        file_time=entry[i] # not used
        i+=1

        file_path=gdrive_info(path, file_id)

        logging.debug("gdrive_info() returned: "+file_path)
        if file_path=="" or not file_path.startswith(remote_root): # gdrive_info says that this is a folder or outside of the remote root, so we ignore it
            continue

        file_path=file_path[len(remote_root):] # remove the remote root prefix
        logging.debug("Change entry to merge: "+file_path)

        logging.info("PROBLEM: sometimes gdrive info cannot find info about file ids that have been deleted") # TODO

        if file_action=="remove":
            del new_data[file_path]
            changes_made=True
            logging.debug("file deleted: "+file_path)
        else:

            # drop the filename from P1. Call the resulting path P2
            dir_path=os.path.dirname(file_path)
            # write to a temp file F1 the lines:
            # + /....P2/**
            # - **

            tf=open(lsl_temp_filter,"w")
            tf.write("+ /"+dir_path+("/" if dir_path!="" else "")+"**\n")
            tf.write("- **\n")
            tf.close()

            # max_depth together with the filter allow us to look at exactly max_depth
            max_depth=file_path.count("/")+1

            # call rclone lsl with the cli path, but provide at the end of the cli --filter-from F1
#            if rclone_lsl(path,tempfile,filter_file,options,path.rstrip("/")+"/"+file_path) == 1:
            if rclone_lsl(path,tempfile,filter_file,options,lsl_temp_filter,max_depth) == 1:
                return 1, orig_token, True, None, None
            status,e=load_list(tempfile)
            if status==1:
                return 1, orig_token, True, None, None
            logging.debug("*****************")
            logging.debug(e)
            logging.debug("*****************")
            # in the output, look for P1
            # if not found, then it means that the file is filtered out
#            cached_data[file_path]=e[next(iter(e))] # rclone lsl returns only the filename when applied to individual files
            logging.debug("file modified: "+file_path)
            if not file_path in e.keys():
                logging.debug("actually, file was filtered out: "+file_path)
                # a filter must have dropped the file
                continue
            logging.debug("***"+str(e[file_path]))
            new_data[file_path]=e[file_path]
            changes_made=True

    if changes_made:
        logging.debug("writing changes to "+ofile+"; contains data for "+file_path+": "+("yes" if file_path in new_data else "no"))
        new_data=collections.OrderedDict(sorted(new_data.items()))
        write_lsl(new_data,ofile) # store sorted dictionary
    
    return 0, token, changes_made, cached_data, new_data


def getPath(service, f):
    parent = f.get('parents')
    path = [{'id': f.get('id'), 'name': f.get('name')}]  # Result
    if parent:
        try:
            while True:
                folder = service.files().get(
                    fileId=parent[0], fields='id, name, parents').execute()
                parent = folder.get('parents')
                if parent is None:
                    break
                path.append({'id': parent[0], 'name': folder.get('name')})
        except Exception as error:
            logging.error("getPath(): some kind of network error occurred")
            logging.info("message:  <{}>".format(e))
            return(None)
    path.reverse()
#    print(path)
    return(path)

def pathToStr(p):
#    print(p)
    if p==None: return(None)
    return("/".join([ d['name'] for d in list(p)]))


def merge_gdrive_changes_python(token, path, lsl_file_cached, ofile, filter_file=None, options=None):
    """
    Fetch an rclone LSL of the path and write it to ofile.
    filter_file is a string full path to a file which will be passed to rclone --filter-from.
    options is a list of switches passed to rclone (not currently used)
    """
    global lsl_temp_filter

    orig_token=token
    linenum = inspect.getframeinfo(inspect.stack()[1][0]).lineno

    service=googleapi_service(path)
    if service==None:
        return 1, orig_token, False, None, None

    changed=[]
    page_token=token
    while page_token is not None:
        response = service.changes().list(pageToken=page_token,spaces='drive',includeRemoved=True,pageSize=1000,fields="nextPageToken,newStartPageToken,changes(fileId,removed,file(id,name,trashed,mimeType,parents))").execute()
        for change in response.get('changes'):
            # Process change
            logging.debug('Change found for file: '+change.get('fileId')+'; removed='+str(change.get('removed')))
            logging.debug('***'+str(change))
            f=change.get('file')
            if f!=None:
                logging.debug("NAME="+f.get('name'))
                logging.debug("TRASHED="+str(f.get('trashed')))
                logging.debug("MIMETYPE="+f.get('mimeType'))

                mime=f.get('mimeType')
                if mime=="application/vnd.google-apps.folder":
                    logging.debug("Folder detected. Ignoring")
                elif mime.startswith("application/vnd.google-apps."):
                    logging.debug("TODO: Google Apps file detected. Ignoring") # TODO
                else:
                    removed=f.get('trashed') or change.get('removed')
                    changed.append([change.get('fileId'),f.get('name'),"remove" if removed else "modify", f ])
            else:
                logging.info("PROBLEM: file "+change.get('fileId')+" does not have file info. Probably it was removed. I should really have a database of fileIds and their former names, so I can delete it locally. Ignoring it for now") # TODO

        if 'newStartPageToken' in response:
            # Last page, save this token for the next polling interval
            saved_start_page_token = response.get('newStartPageToken')
            logging.info("newStartPageToken="+str(saved_start_page_token))
        page_token = response.get('nextPageToken')
        logging.info("nextPageToken="+str(page_token))

    token=saved_start_page_token

    if len(changed)==0:
        return 0, token, False, None, None
        
    # load the cached file
    logging.debug("loading prior file list")
    status, cached_data = get_and_load_lsl(path, lsl_file_cached)
    logging.debug("done loading prior file list")
    if status != 0:
        return 1, orig_token, False, None, None

    remote_root=path.split(":",1)[1]
    if not remote_root.endswith("/"):
        remote_root=remote_root+"/"
    if remote_root=="/":
        remote_root=""
    changes_made=False
    new_data=cached_data.copy()
    for entry in changed:
        i=0
        file_id=entry[i]
        i+=1
        file_name=entry[i] # not used
        i+=1
        file_action=entry[i]
        i+=1
        file_record=entry[i]
        i+=1

        file_path=pathToStr(getPath(service, file_record))
        if file_path==None:
            return 1, orig_token, False, None, None

        logging.debug("pathToStr() returned: "+file_path)
        if file_path=="" or not file_path.startswith(remote_root): # we should drop this
            continue

        file_path=file_path[len(remote_root):] # remove the remote root prefix
        logging.debug("Change entry to merge: "+file_path)

        if file_action=="remove":
            if file_path in new_data:
                del new_data[file_path]
                changes_made=True
                logging.debug("file deleted: "+file_path)
        else:

            # drop the filename from P1. Call the resulting path P2
            dir_path=os.path.dirname(file_path)
            # write to a temp file F1 the lines:
            # + /....P2/**
            # - **

            tf=open(lsl_temp_filter,"w")
            tf.write("+ /"+dir_path+("/" if dir_path!="" else "")+"**\n")
            tf.write("- **\n")
            tf.close()

            # max_depth together with the filter allow us to look at exactly max_depth
            max_depth=file_path.count("/")+1

            # call rclone lsl with the cli path, but provide at the end of the cli --filter-from F1
#            if rclone_lsl(path,ofile,filter_file,options,path.rstrip("/")+"/"+file_path) == 1:
            if rclone_lsl(path,ofile,filter_file,options,lsl_temp_filter,max_depth) == 1:
                return 1, orig_token, False, None, None
            status,e=load_list(ofile)
            if status==1:
                return 1, orig_token, False, None, None
            logging.debug("*****************")
            logging.debug(e)
            logging.debug("*****************")
            # in the output, look for P1
            # if not found, then it means that the file is filtered out
#            cached_data[file_path]=e[next(iter(e))] # rclone lsl returns only the filename when applied to individual files
            logging.debug("file modified: "+file_path)
            if not file_path in e.keys():
                logging.debug("actually, file was filtered out: "+file_path)
                # a filter must have dropped the file
                continue
            logging.debug("***"+str(e[file_path]))
            new_data[file_path]=e[file_path]
            changes_made=True

    if changes_made:
        logging.debug("writing changes to "+ofile+"; contains data for "+file_path+": "+("yes" if file_path in new_data else "no"))
        new_data=collections.OrderedDict(sorted(new_data.items()))
        write_lsl(new_data,ofile) # store sorted dictionary
    
    return 0, token, changes_made, cached_data, new_data

def merge_gdrive_changes(token, path, lsl_file_cached, ofile, filter_file=None, options=None):
    return(merge_gdrive_changes_python(token, path, lsl_file_cached, ofile, filter_file, options))


def rclone_cmd(cmd, p1=None, p2=None, filter_file=None, files_file=None, options=None):
    """
    Execute an rclone command.
    p1 and p2 are optional.
    filter_file is a string full path to a file which will be passed to rclone --filter-from.
    files_file is a string full path to a list of files for the rclone command operation, such as copy these files.
    options is a list of switches passed to rclone
        EG: ["--filter-from", "some_file", "--dry-run", "-vv", '--log-format', '""']
    """
    linenum = inspect.getframeinfo(inspect.stack()[1][0]).lineno
    process_args = [rclone, cmd, "--config", rcconfig]
    if p1 is not None:
        process_args.append(p1)
    if p2 is not None:
        process_args.append(p2)
    if filter_file is not None:
        process_args.extend(["--filter-from", filter_file])
    if files_file is not None:
        process_args.extend(["--files-from-raw", files_file])
    if options is not None:
        process_args.extend(options)
    if args.rclone_args is not None:
        process_args.extend(args.rclone_args)
    logging.debug("    rclone command:  {}".format(process_args))
    for x in range(MAXTRIES):
        try:
            p = subprocess.Popen(process_args)
            p.wait()
            if p.returncode == 0:
                return 0
        except Exception as e:
            logging.info(print_msg("WARNING", "rclone {} try {} failed.".format(cmd, x+1), p1))
            logging.info("message:  <{}>".format(e))
    logging.error(print_msg("ERROR", "rclone {} failed.  (Line {})".format(cmd, linenum), p1))
    return 1


LINE_FORMAT = re.compile(r'\s*([0-9]+) ([\d\-]+) ([\d:]+).([\d]+) (.*)')
def load_list(lslfile):
    """
    Load the content of the lslfile into a dictionary.
    The key is the path to the file relative to the Path1/Path2 base.
    File size of -1, as for Google Docs files, prints a warning and are not loaded.

    lsl file format example:
          size <----- datetime (epoch) ----> key
       3009805 2013-09-16 04:13:50.000000000 12 - Wait.mp3
        541087 2017-06-19 21:23:28.610000000 DSC02478.JPG

    Returned sorted dictionary structure:
        OrderedDict([('RCLONE_TEST', {'size': '110', 'datetime': 946710000.0}),
                     ('file1.txt', {'size': '0', 'datetime': 946710000.0}), ...
    """
    d = {}
    try:
        line = "<none>"
        line_cnt = 0
        with io.open(lslfile, mode='rt', encoding='utf8') as f:
            for line in f:
                line_cnt += 1
                out = LINE_FORMAT.match(line)
                if out:
                    size = out.group(1)
                    date = out.group(2)
                    _time = out.group(3)
                    microsec = out.group(4)
                    # on Android, time.mktime gives error for the hour of the Spring time change, e.g. 03/10/2019 at 2am (any time during 2am)
                    # That's because, tecnically, that hour does not exist. So, if we get an error, we try the next hour, e.g. 3am
                    # If it still gives error, then we fail.
                    #date_time = time.mktime(datetime.strptime(date + ' ' + _time, '%Y-%m-%d %H:%M:%S').timetuple()) + float('.'+ microsec)
                    dt = datetime.strptime(date + ' ' + _time, '%Y-%m-%d %H:%M:%S')
                    try:
                        date_time = time.mktime(dt.timetuple()) + float('.'+ microsec)
                    except OverflowError:
                        date_time = time.mktime((dt+timedelta(hours=1)).timetuple()) + float('.'+ microsec)
                    filename = out.group(5)
                    if filename in d:
                        logging.warning (f"WARNING  Duplicate line in LSL file:   <{line[:-1]}>")
                        strtime = datetime.fromtimestamp(d[filename]['datetime']).strftime("%Y-%m-%d %H:%M:%S.%f")
                        logging.warning (f"         Prior found (keeping latest): <{d[filename]['size']:>9} {strtime}    {filename}>")
                        if date_time > d[filename]['datetime']:
                            d[filename] = {'size': size, 'datetime': date_time, 'orig_datetime': date+" "+_time+"."+microsec}
                    else:
                        d[filename] = {'size': size, 'datetime': date_time, 'orig_datetime': date+" "+_time+"."+microsec}

                else:
                    logging.warning("Something wrong with this line (ignored) in {}.  (Google Doc files cannot be synced.):\n   <{}>".format(lslfile, line))
        return 0, collections.OrderedDict(sorted(d.items()))        # return Success and a sorted list

    except Exception as e:
        logging.error(f"Exception in load_list loading <{lslfile}>:\n  <{e}>\n  Line # {line_cnt}:  {line}")
        return 1, ""                                                # return False


def get_and_load_lsl(path_text, lsl_file, path12=None):
    """
    Optionally call for an rclone lsl of the referenced path12, written to the lsl_file.
    Then load the lsl file content into into a dictionary and return the dictionary to the caller.

    path_text is a convenience string for logging, eg "Path2 prior"
    lsl_file is the full path string for file to be written to by the rclone lsl, and read from for loading the content.
    path12 is a string to be passed to the rclone lsl, eg "Dropbox:"
    """
    if path12 is not None:
        if rclone_lsl(path12, lsl_file, filter_file=user_filter_file):
            return RTN_ABORT, None
    status, loaded_list = load_list(lsl_file)
    if status:
        logging.error(print_msg("ERROR", f"Failed loading {path_text} list file <{lsl_file}>"))
        return RTN_ABORT, None
    if not first_sync and len(loaded_list) == 0:
        logging.error(print_msg("ERROR", f"Zero length in {path_text} list file <{lsl_file}>.  Cannot sync to an empty directory tree."))
        return RTN_CRITICAL, None
    return 0, loaded_list

def incremental_get_and_load_lsl(path_text, change_token, lsl_file_cached, lsl_file, path12=None):
    """
    Optionally call for an rclone lsl of the referenced path12, written to the lsl_file.
    Then load the lsl file content into into a dictionary and return the dictionary to the caller.

    path_text is a convenience string for logging, eg "Path2 prior"
    lsl_file is the full path string for file to be written to by the rclone lsl, and read from for loading the content.
    path12 is a string to be passed to the rclone lsl, eg "Dropbox:"
    """
    if path12 is None:
        logging.error("incremental_get_and_load_lsls() called without path12")
        return RTN_CRITICAL, None, None, False, change_token

    loaded_list = None
    status, change_token, changes_detected, cached_data, loaded_list = merge_gdrive_changes(change_token, path12, lsl_file_cached, lsl_file, filter_file=user_filter_file)
    logging.debug("incremental_get_and_load_lsls(): merging complete")
    if status != 0:
        return RTN_ABORT, None, None, False, change_token

    if changes_detected:
        if loaded_list==None:
            status, loaded_list = load_list(lsl_file)
            if status:
                logging.error(print_msg("ERROR", f"Failed loading {path_text} list file <{lsl_file}>"))
                return RTN_ABORT, None, None, False, change_token
        if not first_sync and len(loaded_list) == 0:
            logging.error(print_msg("ERROR", f"Zero length in {path_text} list file <{lsl_file}>.  Cannot sync to an empty directory tree."))
            return RTN_CRITICAL, None, None, False, change_token
    return 0, loaded_list, cached_data, changes_detected, change_token

def request_lock(caller, lock_file):
    for _ in range(5):
        if os.path.exists(lock_file):
            with io.open(lock_file, mode='rt', encoding='utf8',errors="replace") as fd:
                locked_by = fd.read()
                logging.info("Lock file exists - Waiting a sec: <{}>\n<{}>".format(lock_file, locked_by[:-1]))   # remove the \n
            time.sleep(1)
        else:  
            with io.open(lock_file, mode='wt', encoding='utf8') as fd:
                fd.write("Locked by {} at {}\n".format(caller, time.asctime(time.localtime())))
                logging.info("Lock file created: <{}>".format(lock_file))
            return 0
    logging.warning("Timed out waiting for lock file to be cleared: <{}>".format(lock_file))
    return -1

def release_lock(lock_file):
    if os.path.exists(lock_file):
        logging.info("Lock file removed: <{}>".format(lock_file))
        os.remove(lock_file)
        return 0
    else:
        logging.warning("Attempted to remove lock file but the file does not exist: <{}>".format(lock_file))
        return -1

def keyboardInterruptHandler(signal, frame):
    logging.error("***** KeyboardInterrupt Critical Error Abort - Must run --first-sync to recover.  See README.md *****\n")
    if os.path.exists(path2_lsl_file):
        shutil.move(path2_lsl_file, path2_lsl_file + '_ERROR')
    if os.path.exists(path1_lsl_file):
        shutil.move(path1_lsl_file, path1_lsl_file + '_ERROR')
    release_lock(lock_file)
    sys.exit(2)
signal.signal(signal.SIGINT, keyboardInterruptHandler)
    

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="***** BiDirectional Sync for Cloud Services using rclone *****")
    parser.add_argument('Path1',
                        help="Local path, or cloud service with ':' plus optional path.  Type 'rclone listremotes' for list of configured remotes.")
    parser.add_argument('Path2',
                        help="Local path, or cloud service with ':' plus optional path.  Type 'rclone listremotes' for list of configured remotes.")
    parser.add_argument('-1', '--first-sync', action='store_true',
                        help="First run setup.  WARNING: Path1 files may overwrite path2 versions.  Consider using with --dry-run first.  Also asserts --verbose.")
    parser.add_argument('-c', '--check-access', action='store_true',
                        help="Ensure expected RCLONE_TEST files are found on both path1 and path2 filesystems, else abort.")
    parser.add_argument('--check-filename', default=CHK_FILE, 
                        help=f"Filename for --check-access (default is <{CHK_FILE}>).")
    parser.add_argument('-D', '--max-deletes', type=int, default=MAX_DELETE,
                        help=f"Safety check for percent maximum deletes allowed (default {MAX_DELETE}%%).  If exceeded the rclonesync run will abort.  See --force.")
    parser.add_argument('-F', '--force', action='store_true',
                        help="Bypass --max-deletes safety check and run the sync.  Also asserts --verbose.")
    parser.add_argument('--no-check-sync', action='store_true',
                        help="Disable comparison of final LSL files (default is check-sync enabled).")
    parser.add_argument('--check-sync-only', action='store_true',
                        help="Only execute the comparison of LSL files from the last rclonesync run.")
    parser.add_argument('-e', '--remove-empty-directories', action='store_true',
                        help="Execute rclone rmdirs as a final cleanup step.")
    parser.add_argument('-f','--filters-file', default=None,
                        help="File containing rclone file/path filters (needed for Dropbox).")
    parser.add_argument('-r','--rclone', default="rclone",
                        help="Path to rclone executable (default is rclone in path environment var).")
    parser.add_argument('--config', default=None,
                        help="Path to rclone config file (default is typically ~/.config/rclone/rclone.conf).")
    parser.add_argument('--rclone-args', nargs=argparse.REMAINDER,
                        help="Optional argument(s) to be passed to rclone.  Specify this switch and rclone ags at the end of rclonesync command line.")
    parser.add_argument('-v', '--verbose', action='count', default=0,
                        help="Enable event logging with per-file details.  Specify once for info and twice for debug detail.")
    parser.add_argument('--rc-verbose', action='count',
                        help="Enable rclone's verbosity levels (May be specified more than once for more details.  Also asserts --verbose.)")
    parser.add_argument('-d', '--dry-run', action='store_true',
                        help="Go thru the motions - No files are copied/deleted.  Also asserts --verbose.")
    parser.add_argument('-w', '--workdir', default=os.path.expanduser("~/.rclonesyncwd"),
                        help="Specified working dir - useful for testing.  Default is ~user/.rclonesyncwd.")
    parser.add_argument('--no-datetime-log', action='store_true',
                        help="Disable date-time from log output - useful for testing.")
    parser.add_argument('--no-cleanup', action='store_true',
                        help="Retain working files - useful for debug and testing.")
    parser.add_argument('-V', '--version', action='version', version='%(prog)s ' + __version__,
                        help="Return rclonesync's version number and exit.")
    args = parser.parse_args()
    
    first_sync   =  args.first_sync
    check_access =  args.check_access
    chk_file     =  args.check_filename
    max_deletes  =  args.max_deletes
    verbose      =  args.verbose
    rc_verbose   =  args.rc_verbose
    if rc_verbose == None: rc_verbose = 0
    user_filter_file =  args.filters_file
    rclone       =  args.rclone
    dry_run      =  args.dry_run
    force        =  args.force
    rmdirs       =  args.remove_empty_directories


    # Set up logging
    if not args.no_datetime_log:
        logging.basicConfig(format='%(asctime)s:  %(message)s') # /%(levelname)s/%(module)s/%(funcName)s
    else:
        logging.basicConfig(format='%(message)s')

    if verbose >= 2:
        logging.getLogger().setLevel(logging.DEBUG)             # Log debug detail
    elif verbose>0 or rc_verbose>0 or force or first_sync or dry_run:
        logging.getLogger().setLevel(logging.INFO)              # Log each file transaction
    else:
        logging.getLogger().setLevel(logging.WARNING)           # Log only unusual events.  Normally silent.

    logging.info(f"***** BiDirectional Sync for Cloud Services using rclone ({__version__}) *****")


    # Environment checks
    if is_Windows:
        chcp = subprocess.check_output(["chcp"], shell=True).decode("utf-8")
        err = False
        py_encode_env = ''
        if "PYTHONIOENCODING" in os.environ:
            py_encode_env = os.environ["PYTHONIOENCODING"]
        if "65001" not in chcp:
            logging.error ("ERROR  In the Windows CMD shell execute <chcp 65001> to enable support for UTF-8.")
            err = True
        if py_encode_env.lower().replace('-','') != "utf8":
            logging.error ("ERROR  In the Windows CMD shell execute <set PYTHONIOENCODING=UTF-8> to enable support for UTF-8.")
            err = True
        if err:
            sys.exit(1)


    # Check workdir goodness
    workdir = args.workdir
    if not (workdir.endswith('/') or workdir.endswith('\\')):   # 2nd check is for Windows paths
        workdir += '/'
    try:
        if not os.path.exists(workdir):
            os.makedirs(workdir)
    except Exception as e:
        logging.error(f"ERROR  Cannot access workdir at <{workdir}>.")
        sys.exit(1)


    # Check rclone related goodness
    rcversion_FORMAT = re.compile(r"v?(\d+)\.(\d+).*")
    try:
        rclone_V = subprocess.check_output([rclone, "version"]).decode("utf8").split()[1]
    except Exception as e:
        logging.error(f"ERROR  Cannot invoke <rclone version>.  rclone not installed or invalid --rclone path?\nError message: {e}.\n")
        sys.exit(1)
    out = rcversion_FORMAT.match(rclone_V)
    if out:
        rcversion = int(out.group(1)) + float("0." + out.group(2))
        if rcversion < RCLONE_MIN_VERSION:
            logging.error(f"ERROR  rclone minimum version is v{RCLONE_MIN_VERSION}.  Found version v{rcversion}.")
            sys.exit(1)
    else:
        logging.error(f"ERROR  Cannot get rclone version info.  Check rclone installation - minimum version v{RCLONE_MIN_VERSION}.")
        sys.exit(1)

    rcconfig = args.config
    if rcconfig is None:
        try:  # Extract the second line from the two line <rclone config file> output similar to:
                # Configuration file is stored at:
                # /home/<me>/.config/rclone/rclone.conf
            rcconfig = str(subprocess.check_output([rclone, "config", "file"]).decode("utf8")).split(':\n')[1].strip()
        except Exception as e:
            logging.error(f"ERROR  Cannot get <rclone config file> path.  rclone install problem?\nError message: {e}.\n")
            sys.exit(1)
    if not os.path.exists(rcconfig):
        logging.error(f"ERROR  rclone config file <{rcconfig}> not found.  Check rclone configuration, or invalid --config switch?")
        sys.exit(1)

    try:
        clouds = subprocess.check_output([rclone, "listremotes", "--config", rcconfig]).decode("utf8") .split("\n")[:-1]
    except Exception as e:
        logging.error("ERROR  Cannot get list of known remotes.  Have you run rclone config?")
        sys.exit(1)


    # Set up Path1 / Path2
    def pathparse(path):
        """Handle variations in a path argument.
        Cloud:              - Root of the defined cloud
        Cloud:some/path     - Supported with our without path leading '/'s
        X:                  - Windows drive letter
        X:\\some\\path      - Windows drive letter with absolute or relative path
        some/path           - Relative path from cwd (and on current drive on Windows)
        //server/path       - UNC paths are supported
        On Windows a one-character cloud name is not supported - it will be interprested as a drive letter.
        """
        _cloud = False
        path_base = ''
        if ':' in path:
            if len(path) == 1:                                  # Handle corner case of ':' only passed in
                logging.error("ERROR  Path argument <{}> not a legal path.".format(path))
                sys.exit(1)
            if path[1] == ':' and is_Windows:                   # Windows drive letter case
                path_base = path
                if not path_base.endswith('\\'):                # For consistency ensure the path ends with '/'
                    path_base += '/'
            else:                                               # Cloud case with optional path part
                # path_FORMAT = re.compile(r'([\w-]+):(.*)')
                path_FORMAT = re.compile(r'([ \w-]+):(.*)')
                out = path_FORMAT.match(path)
                if out:
                    _cloud = True
                    cloud_name = out.group(1) + ':'
                    if cloud_name not in clouds:
                        logging.error(f"ERROR  Path argument <{cloud_name}> not in list of configured Clouds: {clouds}.")
                        sys.exit(1)
                    path_part = out.group(2)
                    if path_part:
                        if not (path_part.endswith('/') or path_part.endswith('\\')):    # 2nd check is for Windows paths
                            path_part += '/'
                    path_base = cloud_name + path_part
        else:                                                   # Local path (without Windows drive letter)
            path_base = path
            if not (path_base.endswith('/') or path_base.endswith('\\')):
                path_base += '/'

        if not _cloud:
            if not os.path.exists(path_base):
                logging.error(f"ERROR  Local path parameter <{path_base}> cannot be accessed.  Path error?")
                sys.exit(1)

        return path_base

    path1_base = pathparse(args.Path1)
    path2_base = pathparse(args.Path2)

    # Run the job
    lock_file = os.path.join(tempfile.gettempdir(), 'rclonesync_LOCK_' + (
        path1_base + path2_base).replace(':','_').replace(r'/','_').replace('\\','_'))

    if request_lock(sys.argv, lock_file) == 0:
        status = bidirSync()
        release_lock(lock_file)
        if status == RTN_CRITICAL:
            logging.error("***** Critical Error Abort - Must run --first-sync to recover.  See README.md *****\n")
            if os.path.exists(path2_lsl_file):
                shutil.move(path2_lsl_file, path2_lsl_file + '_ERROR')
            if os.path.exists(path1_lsl_file):
                shutil.move(path1_lsl_file, path1_lsl_file + '_ERROR')
            sys.exit(2)
        if status == RTN_ABORT:
            logging.error("***** Error Abort.  Try running rclonesync again. *****\n")
            sys.exit(1)
        if status == 0:
            logging.info(">>>>> Successful run.  All done.\n")
            sys.exit(0)
    else:
        logging.warning("***** Prior lock file in place, aborting.  Try running rclonesync again. *****\n")
        sys.exit(1)
