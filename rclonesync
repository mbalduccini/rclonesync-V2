#!/usr/bin/env python3
"""BiDirectional Sync using rclone"""

__version__ = "V3.2 201201"                         # Version number and date code

#==========================================================================================================
# Configure rclone, including authentication before using this tool.  rclone must be in the search path.
#
# Chris Nelson, November 2017 - 2020
# Contributions:
#   Hildo G. Jr., e2t, kalemas, and silenceleaf
#
# See README.md for revision history
#
# Known bugs:
#   Many print statements use .format vs. Py3 f strings, only for historical reasons.
#
#==========================================================================================================

import argparse
import sys
import re
import os.path
import io
import platform
import shutil
import subprocess
from datetime import timedelta
from datetime import datetime
import tempfile
import time
import logging
import inspect                                      # For getting the line number for error messages.
import collections                                  # For dictionary sorting.
import hashlib                                      # For checking if the filter file changed and force --first_sync.
import signal                                       # For keyboard interrupt handler
import random

# pip install google-api-python-client
# pip install google_auth_oauthlib
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

import configparser
import json

# Configurations and constants
RCLONE_MIN_VERSION = 1.53
is_Windows = False
is_Linux = False
if sys.platform == "win32":
    is_Windows = True
if sys.platform == "linux":
    is_Linux = True

MAX_DELETE = 50                                     # % deleted allowed, else abort.  Use --force or --max_deletes to override.
CHK_FILE = 'RCLONE_TEST'

RTN_ABORT = 1                                       # Tokens for return codes based on criticality.
RTN_CRITICAL = 2                                    # Aborts allow rerunning.  Criticals block further runs.  See Readme.md.

gdrive="../gdrive/gdrive"
path1_incremental=True
skip_refresh=True # we should avoid the final refresh in case someone adds/removes a file between when we sync and when we refresh
simple_timestamp_copy=False # True -> we use the timestamp of the source; False -> we use the timestamp at the destination after the copy
DISREGARD_MICROSEC=True # Android app does not give us the ms/us info
DOUBLE_CHECK_PATH2=True
COMBINED_LSL=True # True to attempt and speed up merge_gdrive_changes_python() when there are many files
USE_SIMPLE_FILTER_MATCHING=True # True to use simple filter matching, which uses my own interpreter of the rclone filter spec language

# TODO cryptdecode separator. Change if rclone changes
CRYPTDECODE_SEP=" \t "
# TODO cryptdecode failure message. Change if rclone changes
CRYPTDECODE_FAIL="Failed to decrypt"

############################### FILTER-CHECK ###############################

import re
import sys

FILTER_CHECK_DEBUG=False
FILTER_CHECK_DEBUG_REGEX=False

# Set to True to append ** if a directory pattern does not already end with it
FORCE_STARSTAR_END=True

SINGLE_ASTERISK_in="\*([^*])"
SINGLE_ASTERISK_out="[^/]*\\1"
DOUBLE_ASTERISK_in="\*\*"
DOUBLE_ASTERISK_out=".*"

CACHED_FILTER_FILES={}

# For rclone patterns, see https://rclone.org/filtering/.
# Only '*' and '** are allowed
def pattern_to_regex(patt):
	if FORCE_STARSTAR_END and patt.endswith("/"):
		patt=patt+"**"

	# we could use re.escape() but we don't want to have "*" escaped!
	#special_chars=[".","^","$","+","?","\\","|","(",")"]
	special_chars=["."]
	re_patt=patt
	for c in special_chars:
		re_patt=re_patt.replace(c,"\\"+c)
	# If the filter pattern starts with a / then it 
	# only matches at the top level of the directory tree, 
	# relative to the root of the remote (not necessarily 
	# the root of the drive). 
	# If it does not start with / then it is matched 
	# starting at the end of the path/file name but it 
	# only matches a complete path element - it must match 
	# from a / separator or the beginning of the path/file.
	if FILTER_CHECK_DEBUG_REGEX: logging.error("RRR 0"+str(re_patt))
	re_patt=re.sub(SINGLE_ASTERISK_in,SINGLE_ASTERISK_out,re_patt)
	if FILTER_CHECK_DEBUG_REGEX: logging.error("RRR 1"+str(re_patt))
	re_patt=re.sub(DOUBLE_ASTERISK_in,DOUBLE_ASTERISK_out,re_patt)
	if FILTER_CHECK_DEBUG_REGEX: logging.error("RRR 2"+str(re_patt))
	if re_patt.startswith("/"):
		# ensure to match only at the top level
		# ensure to match all the way to the end
		re_patt="^"+re_patt+"$"
		if FILTER_CHECK_DEBUG_REGEX: logging.error("RRR 3a"+str(re_patt))
	else:
		# ensure to match a complete path element
		re_patt="^(("+re_patt+")|(.*/"+re_patt+"))$"
		if FILTER_CHECK_DEBUG_REGEX: logging.error("RRR 3b"+str(re_patt))
	return(re_patt)

def line_to_tuple(l,ignore_case):
	v=l.split(" ",1)
	return(v[0],re.compile(pattern_to_regex(v[1]),flags=re.IGNORECASE if ignore_case else 0))

def match_re_patt(pathname,re_patt):
	return(re_patt.match(pathname)!=None)

# filter_lines: array of pairs
#               Pair item 1 is '+' or '-'
#               Pair item 2 is the compiled regex for a rclone pattern (https://rclone.org/filtering/). Only '*' and '** are allowed
# Returns: True if pathname should be included; False if it should be excluded
def is_pathname_accepted(pathname,filter_lines):
	'''
	Rclone commands are applied to path/file names not directories. 
	The entire contents of a directory can be matched to a filter 
	by the pattern directory/* or recursively by directory/**.
	'''
	if not pathname.startswith("/"):
		pathname="/"+pathname
	for (mode,re_patt) in filter_lines:
		if match_re_patt(pathname,re_patt):
			if FILTER_CHECK_DEBUG: logging.error("RRR match with"+str(re_patt));
			return(mode=="+")
	# Include by default
	return(True)

def is_pathname_accepted_by_filter_file(pathname,filter_file):
	if filter_file==None:
		return(True)
	if FILTER_CHECK_DEBUG: logging.error("RRR=== IN filter-check")
	if filter_file in CACHED_FILTER_FILES:
		filter_lines=CACHED_FILTER_FILES[filter_file]
	else:
		with open(filter_file,"r") as fp:
			filter_lines=[line_to_tuple(l.strip(),False) for l in fp]
		CACHED_FILTER_FILES[filter_file]=filter_lines
	if FILTER_CHECK_DEBUG: logging.error("RRR"+str(filter_lines))
	if FILTER_CHECK_DEBUG:
		x=is_pathname_accepted(pathname,filter_lines)
		logging.error("RRR can-be-used: "+pathname+"="+str(x))
		return(x)
	return(is_pathname_accepted(pathname,filter_lines))

############################################################################


def add_suffix_to_fname(fname,suffix):
    dotpos=fname.rfind(".")
    if dotpos!=-1 and dotpos<(len(fname)-1):
        new_fname=fname[:dotpos]+suffix+fname[dotpos:]
    else:
        new_fname=fname + suffix
    return(new_fname)

#def get_crypt_remote(path):
#    key=path.split(":")[0]
#    creds = None
#    # The file token.json stores the user's access and refresh tokens, and is
#    # created automatically when the authorization flow completes for the first
#    # time.
#    if not os.path.exists(rcconfig):
#        logging.error("PROBLEM: get_crypt_remote() cannot find rclone config file "+rcconfig)
#        return(None)
#    #creds = Credentials.from_authorized_user_file('token.json', SCOPES)
#
#    c=configparser.ConfigParser()
#    with open(rcconfig,'r') as fp:
#        c.read_file(fp)
#        if c[key]['type']!='crypt':
#            loging.error("ERROR: path "+path+" MUST be a Crypt. Type found in rclone is "+c[key]['type'])
#            return("")
#        else:
#            return(c[key]['remote'])

# refresh the timestamp of path1_now[item] for every item in files_copy_files; put it back in path1_now[item]['orig_datetime']
def get_latest_file_info_part1(path_base,files_copy_files,path1_now_updated,path1_now):
    for key in files_copy_files:
        path1_now_updated[key]=path1_now[key]
        #new_data=getFileInfo(key,path1_base,user_filter_file)
    cached_data=getFileInfoBatch(files_copy_files,path_base)
        #if new_data!=None:
        #    path1_now_updated[key]=new_data
    for key in files_copy_files:
        if key in cached_data.keys():
            path1_now_updated[key]=cached_data[key]
    return(path1_now_updated)

# [PART 2] refresh the timestamp of path1_now[item] for every item in files_copy_files; put it back in path1_now[item]['orig_datetime']
def get_latest_file_info_part2(path_base,files_copy_files,path1_now_updated,path2_now_updated,path1_now):
    already_output_file_list=False

    for key in files_copy_files:
        path2_now_updated[key]=path1_now_updated[key]
        #new_data=getFileInfo(key,path2_base,user_filter_file)
    cached_data=getFileInfoBatch(files_copy_files,path_base)
        #if new_data!=None:
        #    path2_now_updated[key]=new_data
    for key in files_copy_files:
        if key in cached_data.keys():
            path2_now_updated[key]=cached_data[key]
        elif not dry_run:
            logging.error("RRR ***BAD!!! we didn't get info back after copying "+key+"!!!")
            if already_output_file_list:
                # Reduce redundant output
                logging.error("RRR files_copy_files already logged and backed up; see earlier log messages")
            else:
                logging.error("RRR path_base="+str(path_base))
                logging.error("RRR files_copy_files="+str(files_copy_files))
                global lsl_temp_filter
                tempfile_XXX=lsl_temp_filter+"_TEMP2"
                shutil.copy(tempfile_XXX, tempfile_XXX+"_bak")
                logging.error("RRR filter backed up to "+tempfile_XXX+"_bak")
                already_output_file_list=True
    return(path2_now_updated)

def get_dir_parents(s):
    l=[]
    if s.endswith("/"): s=s[:-1]
    s=os.path.dirname(s)
    while s!="":
        l.append(s+"/")
        s=os.path.dirname(s)
    return(l)

def escape_filter_chars(s):
    special_rclone_chars=["*","?","[","]","!","{","}"]
    for c in special_rclone_chars:
        s=s.replace(c,"\\"+c)
    return(s)

def mkdir_via_mkdir(path_base,files_copy_dirs,switches):
        for item in files_copy_dirs:
            if rclone_cmd('mkdir', path_base + item, options=switches):
                return True
        return False

def mkdir_via_copy(path1_base,path2_base,files_copy_dirs,files_copy_filename,switches):
    with io.open(files_copy_filename, mode='wt', encoding='utf8') as outf:
        for item in files_copy_dirs:
            outf.write("+ " + escape_filter_chars(item) + "*\n")
        outf.write("- **\n")
    return rclone_cmd('copy', path1_base, path2_base, filter_file=files_copy_filename, options=switches+["--create-empty-src-dirs"])


# TODO rclone aborted with error for filenames with ":" in them. I should find which other characters cause problems. I should also see if I need to escape them in other locations besides do_copies()
def escape_rclone_special_chars(s):
	special_chars=[ ":" ]
	for c in special_chars:
		s=s.replace(c,"\\"+c)
	return s

def do_copies(path1_base,path2_base,files_copy,path1_now_updated,path2_now_updated,path1_now,files_copy_filename,switches):
    global simple_timestamp_copy

    logging.error(print_msg(path1_base, "RRRR  in do_copies()", path2_base)+"==="+str(files_copy))
    path_changes = False
    files_copy_files=[ f for f in files_copy if not f.endswith("/") ]
    if len(files_copy_files) > 0:
        logging.error(print_msg(path1_base, "RRRR  CASE A", path2_base))
        path_changes = True
        with io.open(files_copy_filename, mode='wt', encoding='utf8') as outf:
            for item in files_copy_files:
                outf.write(escape_rclone_special_chars(item) + "\n")
        # refresh the timestamp of path1_now[item] for every item in files_copy_files; put it back in path1_now[item]['orig_datetime']
        if not simple_timestamp_copy:
            path1_now_updated=get_latest_file_info_part1(path1_base,files_copy_files,path1_now_updated,path1_now)
        logging.info(print_msg(path1_base, "  Do queued copies to", path2_base))
        logging.error(print_msg(path1_base, "RRRR  Do queued copies to", path2_base))
        if rclone_cmd('copy', path1_base, path2_base, files_file=files_copy_filename, options=switches):
            # On copy errors, we just abort the process and allow the caller to restart it later
            # return 1, RTN_CRITICAL, 0, 0
            return 1, RTN_ABORT, 0, 0
        # [PART 2] refresh the timestamp of path1_now[item] for every item in files_copy_files; put it back in path1_now[item]['orig_datetime']
        if not simple_timestamp_copy:
            path2_now_updated=get_latest_file_info_part2(path2_base,files_copy_files,path1_now_updated,path2_now_updated,path1_now)
    files_copy_dirs=[ f for f in files_copy if f.endswith("/") ]
    if len(files_copy_dirs) > 0:
        logging.error(print_msg(path1_base, "RRRR  CASE B", path2_base))
        path_changes = True

        # For performance, remove from the list all parents of dirs that are in the list
#        for item in files_copy_dirs[:]: # iterate over a copy of the list
#            if not is_dir_empty(path1_base, item, switches):
#                files_copy_dirs.remove(item)
##            parents=get_dir_parents(item)
##            for p in parents:
##                if p in files_copy_dirs:
##                    files_copy_dirs.remove(p)

        # only run mkdirs for empty leaves, but do collect updated info about all of them
        files_copy_dirs_empty_only=pruneParentDirs(files_copy_dirs)
        files_copy_dirs_empty_only=pruneNonEmptyDirs(files_copy_dirs_empty_only,path1_base,switches)

        if not simple_timestamp_copy:
            path1_now_updated=get_latest_file_info_part1(path1_base,files_copy_dirs,path1_now_updated,path1_now)
        logging.info(print_msg(path1_base, "  Do queued dir copies to ", path2_base))
        logging.error(print_msg(path1_base, "RRR  Do queued dir copies to ", path2_base))

#        if mkdir_via_mkdir(path2_base,files_copy_dirs_empty_only,switches)
        if mkdir_via_copy(path1_base,path2_base,files_copy_dirs_empty_only,files_copy_filename,switches):
            # Just abort the process and allow the caller to restart it later
            #return 1, RTN_CRITICAL, 0, 0
            return 1, RTN_ABORT, 0, 0

        # [PART 2] refresh the timestamp of path1_now[item] for every item in files_copy_files; put it back in path1_now[item]['orig_datetime']
        if not simple_timestamp_copy:
            path2_now_updated=get_latest_file_info_part2(path2_base,files_copy_dirs,path1_now_updated,path2_now_updated,path1_now)


#        # TODO here and in the other places where we do dir copies, consider copying only dirs that are known to be empty
#        with io.open(files_copy_filename, mode='wt', encoding='utf8') as outf:
#            for item in files_copy_dirs:
#                if item.endswith("/"):
#                    outf.write("+ " + item + "**\n")
#            outf.write("- **\n")
#        # refresh the timestamp of path1_now[item] for every item in files_copy_files; put it back in path1_now[item]['orig_datetime']
#        if not simple_timestamp_copy:
#            path1_now_updated=get_latest_file_info_part1(path1_base,files_copy_dirs,path1_now_updated,path1_now)
#        logging.info(print_msg(path1_base, "  Do queued dir copies to ", path2_base))
#        logging.error(print_msg(path1_base, "  Do queued dir copies to ", path2_base))
#        if rclone_cmd('copy', path1_base, path2_base, filter_file=files_copy_filename, options=switches+["--create-empty-src-dirs"]):
#            return RTN_CRITICAL
#        # [PART 2] refresh the timestamp of path1_now[item] for every item in files_copy_files; put it back in path1_now[item]['orig_datetime']
#        if not simple_timestamp_copy:
#            path2_now_updated=get_latest_file_info_part2(path2_base,files_copy_dirs,path1_now_updated,path2_now_updated,path1_now)
    if not args.no_cleanup and os.path.exists(files_copy_filename):
        os.remove(files_copy_filename)
    return(0, path_changes,path1_now_updated,path2_now_updated)

def do_deletes(path_base,files_delete,files_delete_filename,switches):
    path_changes = False
    files_delete_files=[ f for f in files_delete if not f.endswith("/") ]
    files_delete_dirs=[ f for f in files_delete if f.endswith("/") ]
    if len(files_delete_files) > 0:
        path_changes = True
        with io.open(files_delete_filename, mode='wt', encoding='utf8') as outf:
            for item in files_delete_files:
                outf.write(item + "\n")
        logging.info(print_msg("", "  Do queued deletes on", path_base))
        if rclone_cmd('delete', path_base, files_file=files_delete_filename, options=switches):
            # Just abort the process and allow the caller to restart it later
            #return 1,RTN_CRITICAL
            return 1,RTN_ABORT
    if len(files_delete_dirs) > 0:
        path_changes = True
        with io.open(files_delete_filename, mode='wt', encoding='utf8') as outf:
            for item in files_delete_dirs:
                outf.write("+ " + escape_filter_chars(item) + "**\n")
            outf.write("- **\n")
        logging.info(print_msg("", "  Do queued dir deletes on", path_base))
        # This will NOT remove files or dirs containing files. It will remove all empty dirs in the filter and any subdirs of those dirs (as long as they are empty)
        # NOTE: I considered using the files_file parameter (--files-from-raw)
        #       but it didn't work for rmdirs in my experiments. It didn't
        #       remove the folders
        if rclone_cmd('rmdirs', path_base, filter_file=files_delete_filename, options=switches):
            # Just abort the process and allow the caller to restart it later
            #return 1,RTN_CRITICAL
            return 1,RTN_ABORT
    if not args.no_cleanup and os.path.exists(files_delete_filename):
        os.remove(files_delete_filename)
    return(0,path_changes)


def bidirSync():

    global path1_lsl_file, path2_lsl_file
    global path1_lsl_token # TODO
    global lsl_temp_filter
    global lsl_temp_crypt
    lsl_file_base  = workdir + "LSL_" + (path1_base + path2_base).replace(':','_').replace(r'/','_').replace('\\','_')
            # eg:  '/home/<user>/.rclonesyncwd/LSL_<path1_base><path2_base>'
    path1_lsl_file = lsl_file_base + '_Path1'
    path1_lsl_token = lsl_file_base + '_Path1_token'
    path2_lsl_file = lsl_file_base + '_Path2'
    lsl_temp_filter = lsl_file_base + '_temp_filter'
    lsl_temp_crypt = lsl_file_base + '_temp_crypt'

    
    # ***** Check Sync Only *****
    def check_sync():           # Used here and at the end of the flow
        _, path1_contents = load_list(path1_lsl_file)
        _, path2_contents = load_list(path2_lsl_file)

        sync_integrity_fail = False
        for key in path1_contents:
            if key not in path2_contents:
                logging.info(print_msg("ERROR", "Path1 file not found in Path2", key))
                sync_integrity_fail = True
        for key in path2_contents:
            if key not in path1_contents:
                logging.info(print_msg("ERROR", "Path2 file not found in Path1", key))
                sync_integrity_fail = True
        if sync_integrity_fail:
            logging.error("ERROR: The content of Path1 and Path2 are out of sync.  --first-sync required to recover.")
            return 1
        return 0
        
        path1_contents = None
        path2_contents = None

    if args.check_sync_only:
        logging.info(f">>>>> Checking integrity of LSL history files for Path1  <{path1_base}>  versus Path2  <{path2_base}>")
        if check_sync():
            return RTN_CRITICAL
        return 0

    if args.path1_changes_only:
        logging.info(f"Only checking if path1 has changes")
        token_fp=open(path1_lsl_token,"rt")
        change_token=token_fp.readline().rstrip()
        token_fp.close()
        new_change_token=gdrive_changes_now(path1_base)
        logging.info(f"Stored token: "+str(change_token)+"; new token: "+str(new_change_token)+"; changed="+str(new_change_token.isnumeric() and new_change_token!=change_token))
        if new_change_token.isnumeric() and new_change_token!=change_token:
            logging.warning(f"CHANGES DETECTED") # warning because the calling up must be able to see the result regardless of log level
        return 0

    logging.info(f"Synching Path1  <{path1_base}>  with Path2  <{path2_base}>")

    args_string = ''
    for arg in sorted(args.__dict__):
        argvalue = getattr(args, arg)
        if type(argvalue) is int:
            argvalue = str(argvalue)
        if type(argvalue) is bool:
            if argvalue is False:
                argvalue = "False"
            else:
                argvalue = "True"
        if type(argvalue) is list:              # --rclone-args case
            rcargs = '=['
            for item in argvalue:
                rcargs += item + ' '
            argvalue = rcargs[:-1] + ']'
        if argvalue is None:
            argvalue = "None"
        args_string += arg + '=' + argvalue + ', '
    logging.info ("Command args: <{}>".format(args_string[:-2]))


    # ***** Set up --dry-run, and rclone --verbose and --log-format switches *****
    switches = []
    for _ in range(rc_verbose):
        switches.append("-v")
    if dry_run:     # If dry_run, original LSL files are preserved and lsl's are done to the _DRYRUN files.
        switches.append("--dry-run")
        if os.path.exists(path1_lsl_file):
            shutil.copy(path1_lsl_file, path1_lsl_file + '_DRYRUN')
        if os.path.exists(id_filename(path1_lsl_file)):
            shutil.copy(id_filename(path1_lsl_file), id_filename(path1_lsl_file + '_DRYRUN'))
        path1_lsl_file += '_DRYRUN'
        if os.path.exists(path1_lsl_token):
            shutil.copy(path1_lsl_token, path1_lsl_token + '_DRYRUN')
        path1_lsl_token += '_DRYRUN'
        if os.path.exists(path2_lsl_file):          
            shutil.copy(path2_lsl_file, path2_lsl_file + '_DRYRUN')
        path2_lsl_file  += '_DRYRUN'
    if args.no_datetime_log:
        switches.extend(['--log-format', '""'])


    # ***** Handle user_filter_file, if provided *****
    if user_filter_file is not None:
        logging.info("Using filters-file  <{}>".format(user_filter_file))

        if not os.path.exists(user_filter_file):
            logging.error("Specified filters-file file does not exist:  " + user_filter_file)
            return RTN_ABORT #RTN_CRITICAL

        user_filter_file_MD5 = user_filter_file + "-MD5"

        with io.open(user_filter_file, 'rb') as ifile:
            current_file_hash = bytes(hashlib.md5(ifile.read()).hexdigest(), encoding='utf-8')

        stored_file_hash = ''
        if os.path.exists(user_filter_file_MD5):
            with io.open(user_filter_file_MD5, mode="rb") as ifile:
                stored_file_hash = ifile.read()
        elif not first_sync:
            logging.error("MD5 file not found for filters file <{}>.  Must run --first-sync.".format(user_filter_file))
            return RTN_ABORT #RTN_CRITICAL

        if current_file_hash != stored_file_hash and not first_sync:
            logging.error("Filters-file <{}> has chanaged (MD5 does not match).  Must run --first-sync.".format(user_filter_file))
            return RTN_ABORT #RTN_CRITICAL

        if first_sync:
            logging.info("Storing filters-file hash to <{}>".format(user_filter_file_MD5))
            with io.open(user_filter_file_MD5, 'wb') as ofile:
                ofile.write(current_file_hash)


    # ***** first_sync generate path1 and path2 file lists, and copy any unique path2 files to path1 ***** 
    if first_sync:
        logging.info(">>>>> --first-sync copying any unique Path2 files to Path1")

        if path1_incremental:
            path1_lsl_token_new = path1_lsl_token + '_NEW'
            token_fp=open(path1_lsl_token_new,"wt")
            token_fp.write(gdrive_changes_now(path1_base)+"\n")
            token_fp.close()

        path1_lsl_file_new = path1_lsl_file + '_NEW'
        status, path1_now = get_and_load_lsl("current Path1", path1_lsl_file_new, path1_base)
        if status:  return status

        path2_lsl_file_new = path2_lsl_file + '_NEW'
        status, path2_now = get_and_load_lsl("current Path2", path2_lsl_file_new, path2_base)
        if status:  return status

        files_first_sync_copy_P2P1 = []
        for key in path2_now:
            if key not in path1_now or (not key.endswith("/") and path1_now[key]['datetime'] < path2_now[key]['datetime']):
                logging.info(print_msg("Path2", "  --first-sync queue copy to Path1", key))
                files_first_sync_copy_P2P1.append(key)

        files_first_sync_copy_P2P1_filename = lsl_file_base + "_files_first_sync_copy_P2P1"
        path1_now_updated = {}
        path2_now_updated = {}
        status, path1_changes,path2_now_updated,path1_now_updated=do_copies(path2_base,path1_base,files_first_sync_copy_P2P1,path2_now_updated,path1_now_updated,path2_now,files_first_sync_copy_P2P1_filename,switches)
        if status == 1:
            return path1_changes # error code
        # path1_changes,path2_now_updated,path1_now_updated are currently unused
#        if len(files_first_sync_copy_P2P1) > 0:
#            files_first_sync_copy_P2P1_filename = lsl_file_base + "_files_first_sync_copy_P2P1"
#            with io.open(files_first_sync_copy_P2P1_filename, mode='wt', encoding='utf8') as outf:
#                for item in files_first_sync_copy_P2P1:
#                    if not item.endswith("/"):
#                        outf.write(item + "\n")
#            logging.info(print_msg("Path2", "  Do queued first-sync copies to", "Path1"))
#            if rclone_cmd('copy', path2_base, path1_base, files_file=files_first_sync_copy_P2P1_filename, options=switches+["--create-empty-src-dirs"]):
#                return RTN_CRITICAL
#            with io.open(files_first_sync_copy_P2P1_filename, mode='wt', encoding='utf8') as outf:
#                for item in files_first_sync_copy_P2P1:
#                    if item.endswith("/"):
#                        outf.write("+ "+ item + "\n")
#                    outf.write("- **\n")
#                logging.info(print_msg("Path2", "  Do queued dir copies to ", "Path1"))
#                if rclone_cmd('copy', path2_base, path1_base, filter_file=files_first_sync_copy_P2P1_filename, options=switches+["--create-empty-src-dirs"]):
#                    return RTN_CRITICAL
            if not args.no_cleanup:
                os.remove(files_first_sync_copy_P2P1_filename)

        
        logging.info(">>>>> --first-sync synching Path1 to Path2")
        # NOTE:  --min-size 0 added to block attempting to overwrite Google Doc files which have size -1 on Google Drive.  180729
        if rclone_cmd('sync', path1_base, path2_base, filter_file=user_filter_file, options=switches+["--create-empty-src-dirs"] + ['--min-size', '0']):
            return RTN_CRITICAL

        if 1==0 and skip_refresh:
            # skip_refresh currently disabled
            # Copying over _NEW is WRONG. Suppose Path2 is initially empty and we copy over many files from Path1.
            # Copying over _NEW of Path2 will cause the final lsl file to be empty, instead of listing all the
            # files that have been copied to Path2!
            # One option is to leverage the same code that is used in the (not first_sync) branch,
            # however that won't work if some files have been skipped due to errors.
            # Given that first-sync may entail large and long transfers, such errors are possible,
            # and so I think it's much safer to just do a full refresh.
            logging.info(">>>>> --first-sync skipping refreshing lsl files")
            shutil.copy2(path1_lsl_file_new, path1_lsl_file)
            shutil.copy2(path2_lsl_file_new, path2_lsl_file)
        else:
            logging.info(">>>>> --first-sync refreshing lsl files")
#            if rclone_lsl(path1_base, path1_lsl_file, filter_file=user_filter_file):
            if dump_path_listing(path1_base, path1_lsl_file, filter_file=user_filter_file):
                return RTN_CRITICAL
            # Because create_id_list() calls dump_path_listing(), we replace the next two lines
            # with code that leverages the result of the previous call to dump_path_listing().
            #status=create_id_list(path1_base, path1_lsl_file)
            #if status!=0:  return status
            status, cached_data = load_list(path1_lsl_file, user_filter_file)
            if status:
                logging.error(print_msg("ERROR", f"Failed loading current Path1 list file <{path1_lsl_file}>"))
                return RTN_CRITICAL
            status, _ = create_id_list_from_cached_data(path1_base, cached_data, path1_lsl_file)
            if status==False:  return RTN_CRITICAL

#            if rclone_lsl(path2_base, path2_lsl_file, filter_file=user_filter_file):
            if dump_path_listing(path2_base, path2_lsl_file, user_filter_file):
                return RTN_CRITICAL

        # success. We can finalize the change token
        if path1_incremental:
            shutil.copy2(path1_lsl_token_new, path1_lsl_token)

        if not args.no_cleanup:
            os.remove(path1_lsl_file_new)
            os.remove(path2_lsl_file_new)
            if path1_incremental:
                os.remove(path1_lsl_token_new)

        return 0
    

    # ***** Check for existence of prior Path1 and Path2 lsl files *****
    if (not os.path.exists(path1_lsl_file) or not os.path.exists(path2_lsl_file)):
        # On prior critical error abort, the prior LSL files are renamed to _ERROR to lock out further runs
        logging.error("***** Cannot find prior Path1 or Path2 lsl files, likely due to critical error on prior run.")
        return RTN_CRITICAL


    # ***** Check for Path1 deltas relative to the prior sync *****
    logging.info(">>>>> Path1 Checking for Diffs")

    if path1_incremental:
        token_fp=open(path1_lsl_token,"rt")
        change_token=token_fp.readline().rstrip()
        token_fp.close()

    logging.info("[MB] PROBLEM: ASSUMPTION: PATH1 IS THE REMOTE GOOGLE DRIVE. PATH2 IS THE LOCAL PATH") # TODO


    path1_prior = None
    path1_prior_count = None
    path1_may_have_changed=True
    path1_lsl_file_new = path1_lsl_file + '_NEW'
    if path1_incremental:
        status, path1_now, path1_prior, path1_may_have_changed, change_token = incremental_get_and_load_lsl("current Path1", change_token, path1_lsl_file, path1_lsl_file_new, path1_base)
        if status:
            logging.debug("**************incremental_get_and_load_lsl() returned an error")
        else:
            logging.debug("**************SAME!!") if path1_now == path1_prior else logging.debug("***************DIFF!!!")
            logging.debug("**************path1_may_have_changed="+str(path1_may_have_changed))
    else:
        status, path1_now = get_and_load_lsl("current Path1", path1_lsl_file_new, path1_base)
    if status:  return status


    def get_check_files (loaded_lsl):
        check_files = {}
        for key in loaded_lsl:
            if args.check_filename in key and "rclonesync/Test/" not in key:
                check_files[key] = loaded_lsl[key]
        return check_files

    if check_access and path1_may_have_changed:
        path1_check = get_check_files(path1_now)

    if path1_prior == None and path1_may_have_changed:
        logging.debug("Loading Path1 list")
        status, path1_prior = get_and_load_lsl("prior Path1", path1_lsl_file)
        if status:  return status
    if path1_prior != None:
        path1_prior_count = len(path1_prior)    # Save for later max deletes check

    path1_deltas = {}
    path1_deleted = 0
    path1_found_same = True
    if path1_may_have_changed:
        logging.debug("looking for changes in Path1")
        path1_found_same = False
        for key in path1_prior:
            _newer=False; _older=False; _size=False; _deleted=False
            if key not in path1_now:
                logging.info(print_msg("Path1", "  File was deleted", key))
                path1_deleted += 1
                _deleted = True
            # For directories, we can disregard cases in which the directory timestamp has been changed
            elif not key.endswith("/"):
                if path1_prior[key]['datetime'] != path1_now[key]['datetime']:
                    if path1_prior[key]['datetime'] < path1_now[key]['datetime']:
                        logging.info(print_msg("Path1", "  File is newer", key))
                        _newer = True
                    else:               # Current path1 version is older than prior sync.
                        logging.info(print_msg("Path1", "  File is OLDER", key))
                        _older = True
    
            if _newer or _older or _size or _deleted:
                path1_deltas[key] = {'new':False, 'newer':_newer, 'older':_older, 'size':_size, 'deleted':_deleted}
            else:
                path1_found_same = True # Once we've found at least 1 unchanged file we know that not everything has changed, as with a DST time change

        logging.debug("looking for new files in Path1")
        for key in path1_now:
            if key not in path1_prior:
                logging.info(print_msg("Path1", "  File is new", key))
                path1_deltas[key] = {'new':True, 'newer':False, 'older':False, 'size':False, 'deleted':False}
        logging.debug("done looking for new files in Path1")

    path1_prior = None              # Free up the memory
#    path1_now = None

    path1_deltas = collections.OrderedDict(sorted(path1_deltas.items()))    # Sort the deltas list.
    if len(path1_deltas) > 0:
        news = newers = olders = deletes = 0
        for key in path1_deltas:
            if path1_deltas[key]['new']:      news += 1
            if path1_deltas[key]['newer']:    newers += 1
            if path1_deltas[key]['older']:    olders += 1
            if path1_deltas[key]['deleted']:  deletes += 1
        logging.info(f"  {len(path1_deltas):4} file change(s) on Path1: {news:4} new, {newers:4} newer, {olders:4} older, {deletes:4} deleted")


    # ***** Check for Path2 deltas relative to the prior sync *****
    logging.info(">>>>> Path2 Checking for Diffs")

    path2_lsl_file_new = path2_lsl_file + '_NEW'
    using_override=override_filename_exists(path2_lsl_file_new)
    status, path2_now = get_and_load_lsl("current Path2", path2_lsl_file_new, path2_base)
    if status:  return status

    if check_access:
        path2_check = get_check_files(path2_now)

    status, path2_prior = get_and_load_lsl("prior Path2", path2_lsl_file)
    if status:  return status
    path2_prior_count = len(path2_prior)    # Save for later max deletes check

    path2_lsl_file_new_BAD=False
    if using_override and DOUBLE_CHECK_PATH2:
        key_batch=[]
        for key in path2_prior:
            if key not in path2_now:
                key_batch.append(key)
            elif path2_prior[key]['datetime'] != path2_now[key]['datetime']:
                # if we are double-checking, we get the timestamp of the physical file
                # and we use that instead of what we received from the calling app
                key_batch.append(key)
        for key in path2_now:
            if key not in path2_prior:
                key_batch.append(key)
        cached_data=getFileInfoBatch(key_batch,path2_base)
        for key in key_batch:
            # CASES:
            #   1. file exists in path2_now but does not exist in key_batch
            #      ACTION: remove file from path2_now
            #   2. file does not exist in path2_now but exists in key_batch
            #      ACTION: add file from key_batch to path2_now
            #   3. file exists in path2_now and in key_batch but with different timestamps
            #      ACTION: copy file info from key_batch to path2_now
            # CASE 1
            if key in path2_now and key not in cached_data:
                logging.info(print_msg("Path2", "  File does not really exist, removing it from cached list", key))
                logging.error("RRR Path2  File does not really exist, removing it from cached list "+key)
                del path2_now[key]
                path2_lsl_file_new_BAD=True
            # CASE 2
            elif key not in path2_now and key in cached_data:
                logging.info(print_msg("Path2", "  File was not really deleted, fixing cached list with data on physical disk", key+";"+str(cached_data[key])))
                logging.error("RRR Path2  File was not really deleted, fixing cached list with data on physical disk: "+ key+";"+str(cached_data[key]))
                path2_now[key]=cached_data[key]
                path2_lsl_file_new_BAD=True
            # CASE 3
            elif not key.endswith("/") and key in path2_now and key in key_batch and cached_data[key]['datetime'] != path2_now[key]['datetime']:
                logging.info(print_msg("RRR Path2", "  File has different timestamp than the calling app told us. Fixing cached list with data on physical disk", key+";"+str(path2_now[key])+";"+str(cached_data[key])))
                logging.error("RRR Path2  File has different timestamp than the calling app told us. Fixing cached list with data on physical disk: "+ key+";"+str(path2_now[key])+";"+str(cached_data[key]))
                path2_now[key]=cached_data[key]
                path2_lsl_file_new_BAD=True

    path2_deltas = {}
    path2_deleted = 0
    path2_found_same = False
    for key in path2_prior:
        _newer=False; _older=False; _size=False; _deleted=False
        if key not in path2_now:
            logging.info(print_msg("Path2", "  File was deleted", key))
            logging.error(print_msg("RRR Path2", "  File was deleted", key))
            logging.debug(key)
            path2_deleted += 1
            _deleted = True
        # For directories, we can disregard cases in which the directory timestamp has been changed
        elif not key.endswith("/"):
            if path2_prior[key]['datetime'] != path2_now[key]['datetime']:
                if path2_prior[key]['datetime'] < path2_now[key]['datetime']:
                    logging.info(print_msg("Path2", "  File is newer",key+" "+str(path2_prior[key]['datetime'])+" "+str(path2_now[key]['datetime'])))
                    _newer = True
                elif path2_prior[key]['datetime'] != path2_now[key]['datetime']:
                    # We use elif instead of else in case we are double-checking and the timestamp
                    # of the physical file happens to be identical to that in path2_prior
                    # Current Path2 version is older than prior sync.
                    logging.info(print_msg("Path2", "  File is OLDER", key))
                    logging.error("RRR prior key="+str(path2_prior[key])+"; current key="+str(path2_now[key]))
                    _older = True

        if _newer or _older or _size or _deleted:
            path2_deltas[key] = {'new':False, 'newer':_newer, 'older':_older, 'size':_size, 'deleted':_deleted}
        else:
            path2_found_same = True # Once we've found at least 1 unchanged file we know that not everything has changed, as with a DST time change

    for key in path2_now:
        if key not in path2_prior:
            logging.info(print_msg("Path2", "  File is new", key))
            path2_deltas[key] = {'new':True, 'newer':False, 'older':False, 'size':False, 'deleted':False}

    path2_prior = None              # Free up the memory
#    path2_now = None

    path2_deltas = collections.OrderedDict(sorted(path2_deltas.items()))      # Sort the deltas list.
    if len(path2_deltas) > 0:
        news = newers = olders = deletes = 0
        for key in path2_deltas:
            if path2_deltas[key]['new']:      news += 1
            if path2_deltas[key]['newer']:    newers += 1
            if path2_deltas[key]['older']:    olders += 1
            if path2_deltas[key]['deleted']:  deletes += 1
        logging.info(f"  {len(path2_deltas):4} file change(s) on Path2: {news:4} new, {newers:4} newer, {olders:4} older, {deletes:4} deleted")


    # ***** Check access health to the Path1 and Path2 filesystems *****
    if check_access:
        logging.info(">>>>> Checking Path1 and Path2 rclone filesystems access health")

        check_error = False
        if len(path1_check) < 1 or len(path1_check) != len(path2_check):
            logging.error(print_msg("ERROR", "Failed access health test:  <{}> Path1 count {}, Path2 count {}"
                                        .format(chk_file, len(path1_check), len(path2_check)), ""))
            check_error = True

        for key in path1_check:
            if key not in path2_check:
                logging.error(print_msg("ERROR", "Failed access health test:  Path1 key <{}> not found in Path2".format(key), ""))
                check_error = True
        for key in path2_check:
            if key not in path1_check:
                logging.error(print_msg("ERROR", "Failed access health test:  Path2 key <{}> not found in Path1".format(key), ""))
                check_error = True

        if check_error:
            return RTN_CRITICAL

        logging.info(f"  Found <{len(path1_check)}> matching <{args.check_filename}> files on both paths")


    # ***** Check for too many deleted files - possible error condition and don't want to start deleting on the other side !!! *****
    too_many_path1_deletes = False
    if not force and path1_prior_count != None and float(path1_deleted)/path1_prior_count > float(max_deletes)/100:
        logging.error("SAFETY ABORT - Excessive number of deletes (>{}%, {} of {}) found on the Path1 filesystem <{}>.  Run with --force if desired."
                       .format(max_deletes, path1_deleted, path1_prior_count, path1_base))
        too_many_path1_deletes = True

    too_many_path2_deletes = False
    if not force and float(path2_deleted)/path2_prior_count > float(max_deletes)/100:
        logging.error("SAFETY ABORT - Excessive number of deletes (>{}%, {} of {}) found on the Path2 filesystem <{}>.  Run with --force if desired."
                       .format(max_deletes, path2_deleted, path2_prior_count, path2_base))
        too_many_path2_deletes = True

    if too_many_path1_deletes or too_many_path2_deletes:
        return RTN_ABORT


    # ***** Check for all files changed, such as all dates changed due to DST change, to avoid errant copy everything.  See README.md. *****
    if not force and not path1_found_same:
        logging.error(f"SAFETY ABORT - All files were found to be changed on the Path1 filesystem <{path1_base}>.  Something is possibly wrong.  Run with --force if desired.")
        return RTN_ABORT
        
    if not force and not path2_found_same:
        logging.error(f"SAFETY ABORT - All files were found to be changed on the Path2 filesystem <{path2_base}>.  Something is possibly wrong.  Run with --force if desired.")
        return RTN_ABORT
        

    # ***** Determine and apply changes to Path1 and Path2 *****
    files_copy_P1P2 = []
    files_copy_P2P1 = []
    files_delete_P1 = []
    files_delete_P2 = []
    already_handled = {}

    if len(path1_deltas) == 0 and len(path2_deltas) == 0:
        logging.info(">>>>> No changes on Path1 or Path2")
    else:
        logging.info(">>>>> Determining and applying changes")

        for key in path1_deltas:
            if path1_deltas[key]['new'] or path1_deltas[key]['newer'] or path1_deltas[key]['older']:
                if key not in path2_deltas:
                    logging.info(print_msg("Path1", "  Queue copy to Path2", path2_base + key))
                    files_copy_P1P2.append(key)

                elif path2_deltas[key]['deleted']:
                    logging.info(print_msg("Path1", "  Queue copy to Path2", path2_base + key))
                    files_copy_P1P2.append(key)
                    already_handled[key] = 1

                elif path2_deltas[key]['new'] or path2_deltas[key]['newer'] or path2_deltas[key]['older']:
                    if not key.endswith("/"):
                        # exclude directories: two new directories are ok
                        new_key=add_suffix_to_fname(key,"_Path1")
                        logging.warning(print_msg("WARNING", "  New or changed in both paths", key))
                        logging.warning(print_msg("Path1", "  Renaming Path1 copy", path1_base + new_key))

                        if path1_now == None:
                            status, path1_now = load_list(path1_lsl_file, user_filter_file)
                            if status:
                                logging.error(print_msg("ERROR", f"Failed loading {path_text} list file <{lsl_file}>"))
                                return RTN_ABORT

                        if rclone_cmd('moveto', path1_base + key, path1_base + new_key, options=switches):
                            # TODO consider returning RTN_ABORT
                            return RTN_CRITICAL
                        path1_now[new_key]=path1_now[key]
                        path1_now[new_key]['moved-from']=key
                        path1_now[key]['moved-to']=new_key

                        logging.warning(print_msg("Path1", "  Queue copy to Path2", path2_base + new_key))
                        files_copy_P1P2.append(new_key)

                        new_key=add_suffix_to_fname(key,"_Path2")
                        logging.warning(print_msg("Path2", "  Renaming Path2 copy", path2_base + new_key))
                        if rclone_cmd('moveto', path2_base + key, path2_base + new_key, options=switches):
                            # TODO consider returning RTN_ABORT
                            return RTN_CRITICAL
                        path2_now[new_key]=path2_now[key]
                        path2_now[new_key]['moved-from']=key
                        path2_now[key]['moved-to']=new_key
                        logging.warning(print_msg("Path2", "  Queue copy to Path1", path1_base + new_key))
                        files_copy_P2P1.append(new_key)
                    already_handled[key] = 1

            else: # Path1 deleted
                if key not in path2_deltas:
                    logging.info(print_msg("Path2", "  Queue delete", path2_base + key))
                    files_delete_P2.append(key)
                elif path2_deltas[key]['new'] or path2_deltas[key]['newer'] or path2_deltas[key]['older']:
                    logging.info(print_msg("Path2", "  Queue copy to Path1", path1_base + key))
                    files_copy_P2P1.append(key)
                    already_handled[key] = 1
                elif path2_deltas[key]['deleted']:
                    already_handled[key] = 1

        for key in path2_deltas:
            logging.info(print_msg("Path2", "  processing file: ", key))
            if key not in already_handled:
                if path2_deltas[key]['new'] or path2_deltas[key]['newer'] or path2_deltas[key]['older']:
                    logging.info(print_msg("Path2", "  Queue copy to Path1", path1_base + key))
                    files_copy_P2P1.append(key)
                else: # Deleted 
                    logging.info(print_msg("Path1", "  Queue delete", path1_base + key))
                    files_delete_P1.append(key)

#        path1_changes = False
#        path2_changes = False
        path1_now_updated = {}
        path2_now_updated = {}

        status, path1_changes,path2_now_updated,path1_now_updated=do_copies(path2_base,path1_base,files_copy_P2P1,path2_now_updated,path1_now_updated,path2_now,lsl_file_base+"_files_copy_P2P1",switches)
        if status == 1:
            return path1_changes # error code
        status, path2_changes,path1_now_updated,path2_now_updated=do_copies(path1_base,path2_base,files_copy_P1P2,path1_now_updated,path2_now_updated,path1_now,lsl_file_base+"_files_copy_P1P2",switches)
        if status == 1:
            return path2_changes # error code

        status, res=do_deletes(path1_base,files_delete_P1,lsl_file_base+"_files_delete_P1",switches)
        if status == 1:
            return res
        path1_changes=path1_changes or res
        status, res=do_deletes(path2_base,files_delete_P2,lsl_file_base+"_files_delete_P2",switches)
        if status == 1:
            return res
        path2_changes=path2_changes or res

#        files_copy_P1P2 = None      # Free up the memory
#        files_copy_P2P1 = None
#        files_delete_P1 = None
#        files_delete_P2 = None
#        already_handled = None


    # ***** Clean up and check LSL files integrity *****
    if skip_refresh:
        logging.info(">>>>> Skipping refresh of Path1 and Path2 lsl files")
#        if path1_may_have_changed: shutil.copy2(path1_lsl_file_new, path1_lsl_file) # TODO: redo this
        if len(files_copy_P2P1)==0 and len(files_delete_P1)==0:
            if path1_now != None and path1_may_have_changed: # otherwise, there is no new file and no changes
                shutil.copy2(path1_lsl_file_new, path1_lsl_file)
                shutil.copy2(id_filename(path1_lsl_file_new), id_filename(path1_lsl_file))
        else:
            if path1_now == None:
                status, path1_now = load_list(path1_lsl_file, user_filter_file)
                if status:
                    logging.error(print_msg("ERROR", f"Failed loading {path_text} list file <{lsl_file}>"))
                    return RTN_ABORT
                (_,path1_now)=merge_id_list(path1_now,path1_lsl_file)
            for key in files_copy_P2P1:
                if key in path2_now:
                    if 'moved-from' in path2_now[key]:
                        del path2_now[path2_now[key]['moved-from']]
                        del path2_now[key]['moved-to']
                    logging.info("adding to Path1 cache file "+key+"; data="+str(path2_now[key]))
                    # update the time stamps
                    if simple_timestamp_copy:
                        path1_now[key]=path2_now[key]
                    else:
                        path1_now[key]=path1_now_updated[key]
                        path2_now[key]=path2_now_updated[key]
            for key in files_delete_P1:
                logging.info("deleting from Path1 cache file "+key)
                if key in path1_now: del path1_now[key]
            order_and_write_lsl_and_id(path1_now,path1_lsl_file)

        if len(files_copy_P1P2)==0 and len(files_delete_P2)==0:
            if path2_lsl_file_new_BAD:
                # We found that the content of path2_lsl_file_new is not good.
                # It means that we received it from the calling app and that
                # we determined that some of its entries are outdated w.r.t.
                # the physical files.
                # In that case, we have already corrected path2_now
                # and we must save it to disk instead of
                # relying on path2_lsl_file_new, which has not been
                # corrected.
                order_and_write_lsl(path2_now,path2_lsl_file)
            else:
                shutil.copy2(path2_lsl_file_new, path2_lsl_file)
        else:
            if path1_now == None:
                status, path1_now = load_list(path1_lsl_file, user_filter_file)
                if status:
                    logging.error(print_msg("ERROR", f"Failed loading {path_text} list file <{lsl_file}>"))
                    return RTN_ABORT
            for key in files_copy_P1P2:
                if key in path1_now:
                    if 'moved-from' in path1_now[key]:
                        del path1_now[path1_now[key]['moved-from']]
                        del path1_now[key]['moved-to']
                    logging.info("adding to Path2 cache file "+key+"; data="+str(path1_now[key]))
                    # update the time stamps
                    if simple_timestamp_copy:
                        path2_now[key]=path1_now[key]
                    else:
                        path1_now[key]=path1_now_updated[key]
                        path2_now[key]=path2_now_updated[key]
            for key in files_delete_P2:
                logging.info("deleting from Path2 cache file "+key)
                if key in path2_now: del path2_now[key]
            order_and_write_lsl(path2_now,path2_lsl_file)
    else:
        logging.info(">>>>> Refreshing Path1 and Path2 lsl files")
        if len(path1_deltas) == 0 and len(path2_deltas) == 0:
            if path1_may_have_changed: shutil.copy2(path1_lsl_file_new, path1_lsl_file)
            shutil.copy2(path2_lsl_file_new, path2_lsl_file)
        else:
            if path1_changes and not path1_incremental:
                if rclone_lsl(path1_base, path1_lsl_file, filter_file=user_filter_file):
                    return RTN_CRITICAL
            else:
                if path1_may_have_changed: shutil.copy2(path1_lsl_file_new, path1_lsl_file)

            if path2_changes:
                if rclone_lsl(path2_base, path2_lsl_file, filter_file=user_filter_file):
                    return RTN_CRITICAL
            else:
                shutil.copy2(path2_lsl_file_new, path2_lsl_file)

    path1_now = None              # Free up the memory
    path2_now = None

    files_copy_P1P2 = None      # Free up the memory
    files_copy_P2P1 = None
    files_delete_P1 = None
    files_delete_P2 = None
    already_handled = None

    if path1_incremental:
        # Success. We can update the change token
        token_fp=open(path1_lsl_token,"wt")
        token_fp.write(change_token+"\n")
        token_fp.close()

    # Let the calling app know that we have consumed the local filesystem token, if it exists
    if os.path.exists(new_token_filename(path2_lsl_file)):
        shutil.move(new_token_filename(path2_lsl_file), cached_token_filename(path2_lsl_file))

    if not args.no_cleanup:
        if path1_may_have_changed: os.remove(path1_lsl_file_new)
        os.remove(path2_lsl_file_new)

    path1_deltas    = None      # Free up the memory
    path2_deltas    = None

    if not args.no_check_sync and not dry_run:
        logging.info(f">>>>> Checking integrity of LSL history files for Path1  <{path1_base}>  versus Path2  <{path2_base}>")
        if check_sync():
            return RTN_CRITICAL


    # ***** Optional rmdirs for empty directories *****
    if rmdirs:
        logging.info(">>>>> rmdirs Path1")
        if rclone_cmd('rmdirs', path1_base, filter_file=user_filter_file, options=switches):
            return RTN_CRITICAL

        logging.info(">>>>> rmdirs Path2")
        if rclone_cmd('rmdirs', path2_base, filter_file=user_filter_file, options=switches):
            return RTN_CRITICAL

    return 0


# =====  Support functions  ==========================================================
def print_msg(tag, msg, key=''):
    return "  {:9}{:35} - {}".format(tag, msg, key)



# ***** rclone call wrapper functions with retries *****
# If passing --files-from, add to options variable
# ls_command MUST be either "lsl" or "lsd"
MAXTRIES=3
def rclone_lsl_or_lsf(ls_command,path, ofile, filter_file=None, options=None, extra_filter_file=None, max_depth=-1):
    """
    Fetch an rclone LSL of the path and write it to ofile.
    filter_file is a string full path to a file which will be passed to rclone --filter-from.
    options is a list of switches passed to rclone (not currently used)
    """
    linenum = inspect.getframeinfo(inspect.stack()[1][0]).lineno
#    if individual_file_path=="":
    process_args = [rclone, ls_command, path, "--config", rcconfig]
#    else:
#        process_args = [rclone, "lsl", individual_file_path, "--config", rcconfig]
    if filter_file is not None: # and individual_file_path=="":
        process_args.extend(["--filter-from", filter_file])
    if extra_filter_file is not None:
        process_args.extend(["--filter-from", extra_filter_file])
    if max_depth != -1:
        process_args.extend(["--max-depth", str(max_depth)])
    if options is not None:
        process_args.extend(options)
    if args.rclone_args is not None:
        process_args.extend(args.rclone_args)
    logging.debug("    rclone command:  {}".format(process_args))
    logging.error("REMOVE    rclone command:  {}".format(process_args))
    for x in range(MAXTRIES):
        with io.open(ofile, "wt", encoding='utf8') as of:
            if not subprocess.call(process_args, stdout=of):
                return 0
            logging.info(print_msg("WARNING", "rclone lsl try {} failed.".format(x+1)))
    logging.error(print_msg("ERROR", "rclone lsl failed.  Specified path invalid?  (Line {})".format(linenum)))
    return 1


# Convenience method
def rclone_lsl(path, ofile, filter_file=None, options=None, extra_filter_file=None, max_depth=-1):
    return rclone_lsl_or_lsf("lsl",path, ofile, filter_file, options, extra_filter_file, max_depth)

# Convenience method
def rclone_lsf(path, ofile, filter_file=None, options=None, extra_filter_file=None, max_depth=-1):
    return rclone_lsl_or_lsf("lsf",path, ofile, filter_file, options, extra_filter_file, max_depth)


# ***** rclone call wrapper functions with retries *****
# If passing --files-from, add to options variable
# IMPORTANT: if using --files-from, because of how filters work,
#            the dirs MUST end with "/**" or "/*", or their info won't 
#            be reported. See getFileInfoBatch()
MAXTRIES=3
def rclone_lsd(path, ofile, filter_file=None, options=None, extra_filter_file=None, max_depth=-1, append=False):
    """
    Fetch an rclone LSD of the path and APPEND it to ofile.
    Every dir name will have "/" added at the end to distinguish it from files
    The number-of-files column is removed to match the lsl format

    filter_file is a string full path to a file which will be passed to rclone --filter-from.
    options is a list of switches passed to rclone (not currently used)
    """
    linenum = inspect.getframeinfo(inspect.stack()[1][0]).lineno
#    if individual_file_path=="":
    process_args = [rclone, "lsd", path, "--config", rcconfig, "-R"]
#    else:
#        process_args = [rclone, "lsl", individual_file_path, "--config", rcconfig]
    if filter_file is not None: # and individual_file_path=="":
        process_args.extend(["--filter-from", filter_file])
    if extra_filter_file is not None:
        process_args.extend(["--filter-from", extra_filter_file])
    if max_depth != -1:
        process_args.extend(["--max-depth", str(max_depth)])
    if options is not None:
        process_args.extend(options)
    if args.rclone_args is not None:
        process_args.extend(args.rclone_args)
    logging.debug("    rclone command:  {}".format(process_args))
    logging.error("REMOVE!!    rclone command:  {}".format(process_args))
    ofile_LSD=ofile+"_LSD"
    for x in range(MAXTRIES):
        of=io.open(ofile_LSD, "wt", encoding='utf8')
        if not subprocess.call(process_args, stdout=of):
            of.close()
            LINE_FORMAT = re.compile(r'\s*(-?[0-9]+) ([\d\-]+) ([\d:]+)\s\s*(-?[0-9]+) (.*)')
            line_cnt=0
            with io.open(ofile_LSD, mode='rt', encoding='utf8') as f:
                with io.open(ofile, "at" if append else "wt", encoding='utf8') as xf:
                    for line in f:
                        line_cnt += 1
                        out = LINE_FORMAT.match(line)
                        if out:
                            # size = out.group(1)
                            date = out.group(2)
                            _time = out.group(3)
                            # n_files = out.group(4)
                            dirname = out.group(5)
                            if not dirname.endswith("/"):
                                dirname+="/"
                            xf.write("0 "+date+" "+_time+".000000000 "+dirname+"\n")
                        else:
                            logging.warning("Something wrong with this line (ignored) in {}. :\n   <{}>".format(ofile, line))
            return 0
        logging.info(print_msg("WARNING", "rclone lsd try {} failed.".format(x+1)))
        of.close()
    logging.error(print_msg("ERROR", "rclone lsd failed.  Specified path invalid?  (Line {})".format(linenum)))
    return 1

def pruneParentDirs(dirnames):
    new_dirnames=dirnames[:]
    for item in dirnames:
        parents=get_dir_parents(item)
        for p in parents:
            if p in new_dirnames:
                new_dirnames.remove(p)
    logging.error("REMOVE ++++pruneParentDirs(): "+str(len(dirnames))+" --> "+str(len(new_dirnames)))
    return(new_dirnames)


def pruneNonEmptyDirs(dirnames,path,options):
    global lsl_temp_filter

    tempfile=lsl_temp_filter+"_TEMP2"

    e={}
    for f in dirnames:
        if f.endswith("/"): # Drop files if they exist
            d=f.count("/")+1
            e.setdefault(d,[])
            e[d].append(f)
    new_dirnames=dirnames[:]
    for d in e.keys():
        tf=open(lsl_temp_filter,"w")
        for f in e[d]:
            tf.write("+ "+escape_filter_chars(f)+"**\n") # we do want ** here to match complete branches under the dir
        tf.write("- **\n")
        tf.close()
        if rclone_lsf(path, tempfile, filter_file=lsl_temp_filter, options=options, max_depth=d) == 1:
            logging.debug(":::::::::: something went wrong in rclone_lsl("+pnames+")")
        rf=open(tempfile,"r")
        for l in rf:
            l=l.rstrip()
            # TODO I believe I only need to remove l from new_dirnames rather than all parents, because they have already been removed by the call to pruneParentDirs()
            for p in get_dir_parents(l):
                if p in new_dirnames:
                    new_dirnames.remove(p)
        rf.close()
    logging.error("REMOVE ++++pruneNonEmptyDirs(): "+str(len(dirnames))+" --> "+str(len(new_dirnames)))
    return(new_dirnames)


# ***** rclone call wrapper functions with retries *****
# If passing --files-from, add to options variable
# IMPORTANT: if using --files-from, because of how filters work,
#            the dirs MUST end with "/**" or "/*", or their info won't 
#            be reported. See getFileInfoBatch()
MAXTRIES=3
def is_dir_empty(path, pname, options):
    global lsl_temp_filter

    tempfile=lsl_temp_filter+"_dirempty"

    process_args = [rclone, "lsf", path+pname, "--config", rcconfig, "--max-depth", 1 ]
    if options is not None:
        process_args.extend(options)
    if args.rclone_args is not None:
        process_args.extend(args.rclone_args)
    logging.debug("    rclone command:  {}".format(process_args))
    logging.error("REMOVE!!    rclone command:  {}".format(process_args))
    for x in range(MAXTRIES):
        of=io.open(tempfile, "wt", encoding='utf8')
        if not subprocess.call(process_args, stdout=of):
            of.close()
            with io.open(tempfile, mode='rt', encoding='utf8') as f:
                for line in f:
                    return(False)
            return(True)
        logging.info(print_msg("WARNING", "rclone lsf try {} failed.".format(x+1)))
        of.close()
    logging.error(print_msg("ERROR", "rclone lsf failed.  Specified path invalid?  (Line {})".format(linenum)))
    return 1

aux_service={}
def googleapi_service(path):
    global aux_service

    key=path.split(":")[0]
    if key not in aux_service:
        creds = None
        # The file token.json stores the user's access and refresh tokens, and is
        # created automatically when the authorization flow completes for the first
        # time.
        if not os.path.exists(rcconfig):
            logging.error("PROBLEM: googleapi_service() cannot find rclone config file "+rcconfig)
            return((None,None))
        #creds = Credentials.from_authorized_user_file('token.json', SCOPES)

        c=configparser.ConfigParser()
        try:
            with open(rcconfig,'r') as fp:
                c.read_file(fp)
            if c[key]['type']=='crypt':
                return(googleapi_service(c[key]['remote']))
            elif c[key]['type']!='drive':
                loging.error("ERROR: path "+path+" MUST be a Google Drive. Type found in rclone is "+c[key]['type'])
                return((None,None))
            tok=json.loads(c[key]['token'])
            data={}
            data['token']=tok['access_token']
            data['refresh_token']=tok['refresh_token']
            data['token_uri']="https://oauth2.googleapis.com/token"
            data['client_id']=c[key]['client_id']
            data['client_secret']=c[key]['client_secret']
#            data['scopes']=["https://www.googleapis.com/auth/drive"]
            data['expiry']=tok['expiry']

            logging.debug("***"+str(data))
            SCOPES = ['https://www.googleapis.com/auth/drive']
            creds = Credentials.from_authorized_user_info(data, SCOPES)
            # If there are no (valid) credentials available, let the user log in.
            if not creds or not creds.valid:
                if creds and creds.expired and creds.refresh_token:
                    creds.refresh(Request())
                else:
                    logging.error("PROBLEM: googleapi_service() cannot create credentials with the info in "+rcconfig)
                    return((None,None))
#               else:
#                   flow = InstalledAppFlow.from_client_secrets_file(
#                       'credentials.json', SCOPES)
#                   creds = flow.run_local_server(port=0)

#            # Save the credentials for the next run
#            with open('token.json', 'w') as token:
#                token.write(creds.to_json())

            # static_discovery=False required to run within app -- https://stackoverflow.com/questions/73275882/cannot-build-the-google-drive-api-service-in-an-app-bundled-with-pyinstaller
            aux_service[key] = build('drive', 'v3', credentials=creds, static_discovery=False)
        #except HttpError as error:
        except Exception as error:
            # TODO(developer) - Handle errors from drive API.
            logging.error('googleapi_service(): An error occurred: '+str(error))
            logging.error(f'googleapi_service(): likely unable to connect to the authentication server. Is your device connected to the internet?')
            return((None,None))

    return((aux_service[key],path))


def gdrive_changes_now_python(path):
    (service,path_raw)=googleapi_service(path)
    if service==None:
        return("")

    response = service.changes().getStartPageToken().execute()
    return(response.get('startPageToken'))

def gdrive_changes_now(path):
    return(gdrive_changes_now_python(path))

def gdrive_info(path, file_id):
    linenum = inspect.getframeinfo(inspect.stack()[1][0]).lineno
    logging.info("    PROBLEM: not passing a drive path to gdrive in gdrive_info(). Assuming that the configuration already exists.")

    process_args = [gdrive, "info", file_id]
    logging.debug("    rclone command:  {}".format(process_args))
    pout, perr = subprocess.Popen(process_args, stdout=subprocess.PIPE,universal_newlines=True).communicate()
    res=pout.splitlines()
    logging.debug("gdrive info="+str(res))

    if len(res)<1 or res[0].startswith("Failed"): # some error, e.g. gdrive is unable to find the file info
        logging.debug("    gdrive_info(): gdrive info for "+file_id+" failed with error: "+str(res))
        return("")

    # drop the "line-type: " prefixes
    res=[l.split(": ",1)[1] for l in res]

    fid=res[0] # unused
    name=res[1] # unused
    path=res[2]
    mime=res[3]
    ctime=res[4] # unused
    mtime=res[5] # unused
    shared_flag=res[6] # unused
    viewurl=res[7] # unused

    logging.debug("TODO: support Google Apps files") # TODO
    logging.debug("mime="+mime) # TODO
    return(path if mime!="application/vnd.google-apps.folder" and not mime.startswith("application/vnd.google-apps.") else "")

def write_lsl(data,ofile):
    with io.open(ofile, "wt", encoding='utf8') as of:
        for p in data:
            of.write(data[p]['size']+" "+data[p]['orig_datetime']+" "+p+"\n")

def order_and_write_lsl(data,ofile):
    ordered=collections.OrderedDict(sorted(data.items()))
    write_lsl(ordered,ofile)
    return(ordered)

#
# FORMAT:
#   <id> <plaintext pathname>
#
def write_lsl_id(data,ofile):
    with io.open(ofile, "wt", encoding='utf8') as of:
        for p in data:
            if 'id' in data[p]:
                of.write(data[p]['id']+" "+p+"\n")

def order_and_write_lsl_and_id(data,ofile):
    ordered=collections.OrderedDict(sorted(data.items()))
    write_lsl(ordered,ofile)
    write_lsl_id(data,id_filename(ofile))
    return(ordered)

# TODO pass variable "switches" and pass it as options to rclone_lsl
def getFileInfo(pname,path):
    if rclone_lsl(path+pname,tempfile) == 1:
        logging.debug(":::::::::: something went wrong in rclone_lsl("+pname+")")
    status,e=load_list(tempfile)
    if status==1:
        return None
    if not pname in e.keys():
        return None
    return e[pname]

# TODO pass variable "switches" and pass it as options to rclone_lsl
def getFileInfoBatch(pnames,path):
    global lsl_temp_filter

    tempfile=lsl_temp_filter+"_TEMP2"

    e={}
    files=[ f for f in pnames if not f.endswith("/") ]
    if files!=[]:
        tf=open(lsl_temp_filter,"w")
        for f in files:
            tf.write(f+"\n")
        tf.close()
        if rclone_lsl(path,tempfile,None,["--files-from",lsl_temp_filter]) == 1:
            logging.debug("RRR :::::::::: something went wrong in rclone_lsl("+str(pnames)+")")
        status,e=load_list(tempfile)
        if status==1:
            return None

    # We must add "/**" or "/*" after the dir names because of how filters work.
    # Otherwise, rclone lsd does not return info about (most of!) them
    dirs=[ f for f in pnames if f.endswith("/") ]
    if dirs!=[]:
        tf=open(lsl_temp_filter,"w")
        for f in dirs:
            tf.write(f+"*\n")
            logging.error("R14 "+f+"*")
        tf.close()
        if rclone_lsd(path,tempfile,None,["--files-from",lsl_temp_filter]) == 1:
            logging.debug(":::::::::: something went wrong in rclone_lsl("+pnames+")")
        status,e2=load_list(tempfile)
        if status==1:
            return None
        # TODO not critical to performance, but I should remove from e2 all entries that of which there is not a prefix in dirs. That is because lsd returns also info for the parents of the folders in dirs
        e.update(e2)


    logging.error("RR*****************")
    logging.error("RR "+str(e))
    logging.error("RR*****************")
    return(e)


def getRootId(service):
    root_info = service.files().get(fileId="root", fields='id, name, parents').execute()
    return(root_info.get('id'))

def escape_single_quote(s):
    return(s.replace("'","\\'"))

cachedRootID=None
cachedIdFromPathname={}
def getIdFromPathname(service,p):
    global cachedRootID

    if p.startswith("/"):
        p=p[1:]

    parts=p.split('/')

    if cachedRootID==None:
        cachedRootID=getRootId(service)
    rootID=cachedRootID

    currPath=""
    currID=rootID
    for t in parts:
        newPath=t if currPath=="" else currPath+"/"+t
        if newPath in cachedIdFromPathname:
            currID=cachedIdFromPathname[newPath]
        else:
            # logging.error("RRR '"+currID+"' in parents and name='"+t+"'")
            found=False
            for n_tries in range(0,5):
                try:
                    found = service.files().list(q="'"+currID+"' in parents and name='"+escape_single_quote(t)+"'").execute()
                    if len(found.get('files'))==0:
                        return(None)
                    item=found.get('files')[0]
                    currID=item.get('id')
                    cachedIdFromPathname[newPath]=currID
                    found=True
                    break
                except:
                    # https://stackoverflow.com/questions/15685335/google-drive-files-list-500-error
                    logging.info("Exception while getting listing for q='"+currID+"' in parents and name='"+t+"'")
                    logging.info("newPath="+newPath)
                    logging.info("p="+p)
                    logging.info("Try number "+str(n_tries+1))
                    logging.info("Exception: "+str(sys.exc_info()))
                    time.sleep( (2 ** n_tries) + (random.randint(0,1000)/1000))

            if not found: return(False)
        currPath=newPath
    return(currID)

ID_cache={}
def getPath(service, f):
    global ID_cache
    parent = f.get('parents')
    path = [{'id': f.get('id'), 'name': f.get('name')}]  # Result
    if parent:
        try:
            while True:
                old_id=parent[0]
                if old_id in ID_cache.keys():
                    (name,parent)=ID_cache[old_id]
                    path.append({'id': parent[0], 'name': name})
                else:
                    folder = service.files().get(
                        fileId=old_id, fields='id, name, parents').execute()
                    parent = folder.get('parents')
                    if parent is None:
                        break
                    path.append({'id': parent[0], 'name': folder.get('name')})
                    ID_cache[old_id]=(folder.get('name'),parent)
        except Exception as e:
            logging.error("getPath(): some kind of network error occurred")
            logging.error("getPath(): the file info was: "+str(f))
            logging.info("message:  <{}>".format(e))
            return(None)
    path.reverse()
    return(path)

def pathToStr(p):
    if p==None: return(None)
    return("/".join([ d['name'] for d in list(p)]))

def fetch_file_info(path,info,file_list,is_folders,maxdepth,ofile,filter_file,options):
    global lsl_temp_filter

    changes_made=False

    # TODO use files_file (aka --files-from) for files
    tf=open(lsl_temp_filter,"w")
    for file_path in file_list:
        if USE_SIMPLE_FILTER_MATCHING and not is_pathname_accepted_by_filter_file(file_path,filter_file):
            # file was filtered out
            continue
        tf.write("+ "+escape_filter_chars(file_path)+("*" if is_folders else "")+"\n")
        logging.error("R25 + "+escape_filter_chars(file_path)+("*" if is_folders else ""))
    logging.error("R25 - **")
    tf.write("- **\n")
    tf.close()

    if USE_SIMPLE_FILTER_MATCHING:
        if is_folders:
            if rclone_lsd(path,ofile,None,options,lsl_temp_filter,maxdepth) == 1:
                return((None,info))
        else:
            if rclone_lsl(path,ofile,None,options,lsl_temp_filter,maxdepth) == 1:
                return((None,info))
    else:
        if is_folders:
            if rclone_lsd(path,ofile,filter_file,options,lsl_temp_filter,maxdepth) == 1:
                return((None,info))
        else:
            if rclone_lsl(path,ofile,filter_file,options,lsl_temp_filter,maxdepth) == 1:
                return((None,info))
    status,e=load_list(ofile)
    if status==1:
        return((None,info))
    logging.debug("*****************")
    logging.debug(e)
    logging.debug("*****************")
    logging.error("RRR*****************")
    logging.error(e)
    logging.error("RRR*****************")

    # in the output, look for P1
    # if not found, then it means that the file is filtered out
    for file_path in file_list:
        logging.debug("file modified: "+file_path)
        if not file_path in e.keys():
            logging.debug("actually, file was filtered out: "+file_path)
            logging.error("RRR actually, file was filtered out: "+file_path)
            # a filter must have dropped the file
            continue
        logging.debug("***"+str(e[file_path]))
        info[file_path]=e[file_path]
        changes_made=True
    return((changes_made,info))

crypt_cache={}
decrypt_cache={}
def decrypt_rclone_path(file_path,path_remote,reverse=False):
    global lsl_temp_crypt
    global CRYPTDECODE_SEP
    global CRYPTDECODE_FAIL
    global crypt_cache,decrypt_cache

    if reverse:
        cache1=crypt_cache
        cache2=decrypt_cache
        opts=['--reverse']
    else:
        cache1=decrypt_cache
        cache2=crypt_cache
        opts=[]
    if file_path in cache1:
        file_path=cache1[file_path]
    else:
        
        res=rclone_cmd_read_output("cryptdecode", lsl_temp_crypt, path_remote, file_path, options=opts)
        if res==1:
            return(None)
        crypt_fp=open(lsl_temp_crypt,"rt")
        l=crypt_fp.readline().rstrip()
        crypt_fp.close()
        p=l.index(CRYPTDECODE_SEP)
        if p==-1:
            return(None)
        new_file_path=l[p+len(CRYPTDECODE_SEP):]
        if new_file_path==CRYPTDECODE_FAIL:
            return(None)
        cache1[file_path]=new_file_path
        cache2[new_file_path]=file_path
        file_path=new_file_path
    return(file_path)

def is_remote_crypt(path):
    (service,path_raw)=googleapi_service(path)
    if service==None:
        return False
    return(path!=path_raw)

def get_remote_root(path):
    (service,path_raw)=googleapi_service(path)
    if service==None:
        return ""
    remote_root=path_raw.split(":",1)[1]
    if not remote_root.endswith("/"):
        remote_root=remote_root+"/"
    if remote_root.startswith("/"):
        remote_root=remote_root[1:]
    return(remote_root)


def id_filename(lsl_file):
    return(lsl_file+"_ID")

#
# FORMAT:
#   <id> <plaintext pathname>
#
def create_id_list_from_cached_data(path, cached_data, lsl_file, skip_write=False):
    remote_type_crypt=is_remote_crypt(path)
    (service,_)=googleapi_service(path)
    if service==None:
        return False, []

    path_remote=path.split(":",1)[0]+":"

    invalid_paths=[]
    i=0
    for key in cached_data.keys():
        if i % 20 == 0:
            logging.error("RRR at i="+str(i))
            logging.debug("at i="+str(i))
        i+=1
        fname=key
        if fname.endswith("/"): # drop "/" to reflect the way Google encodes folder names
            fname=fname[:-1]
        # logging.error("RRR "+fname)
        if remote_type_crypt:
            fname=decrypt_rclone_path(fname,path_remote,reverse=True)
            #logging.error("ff2 "+fname)

        #logging.error("ppp "+get_remote_root(path)+fname)
        file_id=getIdFromPathname(service,get_remote_root(path)+fname)
        if file_id==False:
            return False, []
        if file_id==None:
            invalid_paths.append(key)
        else:
            cached_data[key]['id']=file_id # TODO this is useless right now

    if not skip_write:
        write_lsl_id(cached_data,id_filename(lsl_file))
    return True, invalid_paths

def create_id_list(path, lsl_file):
    status, cached_data = get_and_load_lsl("current Path1", lsl_file, path)
    if status:  return status
    status, _ = create_id_list_from_cached_data(path, cached_data, lsl_file)
    if status==False:  return RTN_ABORT
    return 0


def merge_id_list(cached_data, lsl_file):
    cached_id_list={}
    if os.path.exists(id_filename(lsl_file)):
        try:
            with io.open(id_filename(lsl_file), mode='rt', encoding='utf8') as f:
                for line in f:
                    parts=line.rstrip('\r\n').split(" ",1)
                    file_id=parts[0]
                    file_name=parts[1]
                    if not file_name in cached_data:
                        continue
                    cached_data[file_name]['id']=file_id
                    cached_id_list[file_id]=file_name
                    #logging.error("RRR "+file_name)
                    #logging.error("RRR --> "+str(cached_data[file_name]))
        except Exception as e:
            logging.error(f"Exception in merge_id_list loading <{id_filename(lsl_file)}>:\n  <{e}>\n")
    return((cached_id_list,cached_data))

# OUTPUT: added_dirs, added_files, changes_made, new_data
def update_newdate_or_caches(COMBINED_LSL,file_id,file_path,file_is_folder,path,ofile,filter_file,options,added_dirs,added_files,changes_made,new_data):
    if COMBINED_LSL:
        d=file_path.count("/")+1
        if file_is_folder:
            logging.error("RRR APPENDING "+file_path)
            added_dirs.setdefault(d,[])
            added_dirs[d].append((file_id,file_path+"/"))
        else:
            added_files.setdefault(d,[])
            added_files[d].append((file_id,file_path))
    else: # not COMBINED_LSL
        ## drop the filename from P1. Call the resulting path P2
        #dir_path=os.path.dirname(file_path)
        ## write to a temp file F1 the lines:
        ## + /....P2/**
        ## - **

        # old, less efficient approach <<< actually useful to prune the search
        # max_depth together with the filter allow us to look at exactly max_depth
        max_depth=file_path.count("/")+1

        changes_made,new_data=fetch_file_info(path,new_data,[file_path],file_is_folder,max_depth,ofile,filter_file,options)
        if changes_made==None:
            return added_dirs, added_files, changes_made, new_data
        new_data[file_path]['id']=file_id
    return added_dirs, added_files, changes_made, new_data

def remove_root_and_decrypt_pathname_as_needed(file_path,remote_root,path_remote,remote_type_crypt):
        logging.error("HERE7 remote_root="+str(remote_root)) # DEBUG
        file_path=file_path[len(remote_root):] # remove the remote root prefix
        if remote_type_crypt:
            logging.debug("Converting "+file_path+" to decrypted name")
            logging.error("CONVERTING "+file_path)
            file_path=decrypt_rclone_path(file_path,path_remote)
            logging.debug("Decrypted path is "+str(file_path))
        return file_path

# Returns new_data
def move_contained_objects(file_path,file_is_folder,moved_file,remote_root,path_remote,remote_type_crypt,filter_file,cached_data,new_data):
    if not file_is_folder:
        return new_data

    file_path=remove_root_and_decrypt_pathname_as_needed(file_path,remote_root,path_remote,remote_type_crypt)
    if file_path==None:
        return new_data
    file_path_adj=file_path+"/" if file_is_folder else file_path # Could be simplified since we know file_path is a folder

    logging.debug("pathname moved is a folder. Moving all contained objects. "+moved_file)
    logging.error("RRRR pathname moved is a folder. Moving all contained objects. "+moved_file) # DEBUG
    for p in cached_data.keys():
        # Ensure the object is contained in moved_file
        if p==moved_file or not p.startswith(moved_file):
            continue

        # One could in principle check p against the filter, but if it's in cached_data, it means that at least in the past it was not filtered out, and so it's ok to remove it from cached_data anyway I think
#        if USE_SIMPLE_FILTER_MATCHING and not is_pathname_accepted_by_filter_file(p,filter_file):
#            logging.debug("Filtered out by simple filtered matching: "+p)
#        else:
        logging.debug("Causing local removal of: "+p)
        logging.error("RRRR Causing local removal of: "+p)
        #logging.error("RRRR **************MUST STILL ENABLE THE LINE BELOW!!!")
        if p in new_data:
            del new_data[p]

        if not file_path_adj.startswith(remote_root): # skip creation if file was moved out of our remote
            continue

        new_p=p.replace(moved_file,file_path_adj)
        logging.error("RRRR p="+p)
        logging.error("RRRR moved_file="+moved_file)
        logging.error("RRRR p.startswith(moved_file)="+str(p.startswith(moved_file)))
        logging.error("RRRR p.replace(moved_file,file_path_adj)="+new_p)
        logging.error("RRRR p.replace(moved_file,file_path_adj) in new_data="+str(p.replace(moved_file,file_path_adj) in new_data))
        if new_p not in new_data:
            if USE_SIMPLE_FILTER_MATCHING and not is_pathname_accepted_by_filter_file(new_p,filter_file):
                logging.debug("Filtered out by simple filtered matching: "+new_p)
                continue
            logging.debug("implementing move of contained pathname "+p+" to "+new_p)
            logging.error("RRRR implementing move of contained pathname: "+p+" to "+new_p)
            logging.error("RRRR p should be removed from new_data and p.replace(...) should be added to new_data")

            logging.debug("Causing local creation of: "+p)
            logging.error("RRRR Causing local creation of: "+p)
            #logging.error("RRRR **************MUST STILL ENABLE THE LINE BELOW!!!")
            new_data[new_p]=cached_data[p]

    return new_data

def merge_gdrive_changes_python(token, path, lsl_file_cached, ofile, filter_file=None, options=None):
    """
    Fetch an rclone LSL of the path and write it to ofile.
    filter_file is a string full path to a file which will be passed to rclone --filter-from.
    options is a list of switches passed to rclone (not currently used)
    """
    global lsl_temp_filter
    global do_create_id_list

    added_files={}
    added_dirs={}

    orig_token=token
    linenum = inspect.getframeinfo(inspect.stack()[1][0]).lineno

    (service,path_raw)=googleapi_service(path)
    if service==None:
        return 1, orig_token, False, None, None
    remote_type_crypt=is_remote_crypt(path)

    #changed=[]
    changed_dict={}
    page_token=token
    try:
        while page_token is not None:
            # TODO I had a case in which a Windows user put a slash in the GDrive filename. The character was encoded as "". However, the changes() API automatically returns it as "/". As a result, "rclone lsl" + filters is unable to find the file, because rclone somehow does not do that conversion
            response = service.changes().list(pageToken=page_token,spaces='drive',restrictToMyDrive=True,includeRemoved=True,pageSize=1000,fields="nextPageToken,newStartPageToken,changes(fileId,removed,file(id,name,trashed,mimeType,md5Checksum,parents))").execute()
            for change in response.get('changes'):
                # Process change
                logging.debug('Change found for file: '+change.get('fileId')+'; removed='+str(change.get('removed')))
                logging.debug('***'+str(change))
                f=change.get('file')
                if f!=None:
                    logging.error("NAME="+f.get('name')) ###
                    logging.debug("NAME="+f.get('name'))
                    logging.debug("TRASHED="+str(f.get('trashed')))
                    logging.error("TRASHED="+str(f.get('trashed')))
                    logging.debug("MIMETYPE="+f.get('mimeType'))
                    logging.debug("MD5="+str(f.get('md5Checksum')))

                    name=f.get('name')
                    mime=f.get('mimeType')
                    is_folder=(mime=="application/vnd.google-apps.folder")
                    if not is_folder and mime.startswith("application/vnd.google-apps."):
                        logging.debug("TODO: Google Apps file detected. Ignoring") # TODO
                    else:
                        if is_folder:
                            logging.debug("Folder detected: "+name)
                            logging.error("FOLDER: "+name)
                        removed=f.get('trashed') or change.get('removed')
#                        changed.append([change.get('fileId'),name,"remove" if removed else "modify", f, (mime=="application/vnd.google-apps.folder") ])
                        logging.error("RRR removed="+str(removed))
                        changed_dict[change.get('fileId')]=[change.get('fileId'),name,"remove" if removed else "modify", f, (mime=="application/vnd.google-apps.folder") ]
                else:
                    logging.info("PROBLEM: file "+change.get('fileId')+" does not have file info. Probably it was removed. I should really have a database of fileIds and their former names, so I can delete it locally. Ignoring it for now") # TODO
    
            if 'newStartPageToken' in response:
                # Last page, save this token for the next polling interval
                saved_start_page_token = response.get('newStartPageToken')
                logging.info("newStartPageToken="+str(saved_start_page_token))
            page_token = response.get('nextPageToken')
            logging.info("nextPageToken="+str(page_token))
    except Exception as e:
        logging.error("ERROR: merge_gdrive_changes_python() call to service.changes().list() failed")
        logging.info("message:  <{}>".format(e))
        return 1, orig_token, False, None, None

    token=saved_start_page_token

    changed=changed_dict.values()
    if len(changed)==0:
        return 0, token, False, None, None

    # load the cached file
    logging.debug("loading prior file list")
    status, cached_data = get_and_load_lsl(path, lsl_file_cached)
    logging.debug("done loading prior file list")
    logging.error("HERE1") # DEBUG
    if status != 0:
        return 1, orig_token, False, None, None
    (cached_id_list,cached_data)=merge_id_list(cached_data, lsl_file_cached)

    remote_root=get_remote_root(path)
    path_remote=path.split(":",1)[0]+":"

    changes_made=False
    new_data=cached_data.copy()

    if do_create_id_list:
        logging.error("HERE1b") # DEBUG
        status, invalid_paths=create_id_list_from_cached_data(path, cached_data, lsl_file_cached)
        if status==False: return 1, orig_token, False, None, None
        logging.error("HERE1c") # DEBUG
        for p in invalid_paths:
            del new_data[p]
            changes_made=True

    for entry in changed:
        i=0
        file_id=entry[i]
        i+=1
        file_name=entry[i] # not used
        i+=1
        file_action=entry[i]
        i+=1
        file_record=entry[i]
        i+=1
        file_is_folder=entry[i]
        i+=1

        # Find current pathname for file id
        logging.error("HERE2 "+str(file_record)) # DEBUG
        file_path=pathToStr(getPath(service, file_record))
        logging.error("HERE3 "+str(file_path)) # DEBUG

        # Find prior pathname for file id
        prior_path=None
        if file_id in cached_id_list:
            prior_path=cached_id_list[file_id]
            if USE_SIMPLE_FILTER_MATCHING and not is_pathname_accepted_by_filter_file(prior_path,filter_file):
                # do nothing. Prior file will be ignored
                logging.debug("Prior file filtered out by simple filtered matching: "+prior_path)
                prior_path=None
            else:
                if prior_path.endswith("/"): # drop "/" to reflect the way Google encodes folder names
                    prior_path=prior_path[:-1]
                if remote_type_crypt:
                    prior_path=remote_root+decrypt_rclone_path(prior_path,path_remote,reverse=True)
                else:
                    prior_path=remote_root+prior_path
                logging.error("HERE3b "+str(prior_path)) # DEBUG
        logging.error("RRRR PROBLEM: we are not handling the case in which a folder was moved from a filtered-out location to an in-filter location. See the todos around this debug message")
        # Note: at this point, prior_path has no trailing "/" even if it is a folder
        # Check if file has been moved, which we reduce to delete + create
        # TODO figure the following out: if the prior location of the file/folder was outside of the filters, then it was not in cached_id_list. Hence, prior_path is None, and thus we don't have a way to tell that it was moved. That is a BIG problem. Look at the code I created in MY-CURRENT-NOTES.txt. I think I should use that code for folders REGARDLESS whether it is a move OR prior_path==None. Performance shouldn't be too bad, since it's either a folder being moved, or a folder being created. In the former case, performance is what it is anyway. In the latter case, there is nothing inside and so the code I suggested will be very quick. Also see the TODO below "if moved_file in new_data", since it is linked to this one
        if prior_path!=None and file_path!=prior_path and file_action!="remove":
            # File was moved from original location. Break it down in delete + create
            moved_file=None
            if file_id in cached_id_list:
                moved_file=cached_id_list[file_id]
            logging.error("HERE3c "+str(moved_file)) # DEBUG
            # No need to append "/" to moved_file. It is already there since we read the path from cache
            #file_path_adj=moved_file+"/" if file_is_folder else moved_file
            if moved_file!=None and moved_file in new_data:
                # TODO if it is a folder and "moved_file in new_data" is false, then I should use the code that I put in MY-CURRENT-NOTES.txt. See the TODO above "if prior_path!=None and file_path!=prior_path ...", because it is linked to this one.
                # When a folder is moved, Google does NOT send updates for the individual contained files. So, we take care of them ourselves
                new_data=move_contained_objects(file_path,file_is_folder,moved_file,remote_root,path_remote,remote_type_crypt,filter_file,cached_data,new_data)
                del new_data[moved_file]
                changes_made=True
                logging.debug("file deleted by move: "+moved_file)
                logging.error("RRRR file deleted by move: "+moved_file)
        if file_path==None:
            # File no longer in remote?
# TODO figure out why we would behave differently for deleted files below. The only case in which pathToStr(getPath()) returns None seems to be a network issue, in which case it makes sense to abort and not leave the file there
#            if file_record.get('trashed'):
#                logging.debug("PROBLEM: file "+file_name+" was moved to trash and we cannot find its parent. Disregarding it for now rather than aborting. Probably, we'll have a file left behind in the local filesystem")
#                continue
            logging.error("HERE4!!!!!") # DEBUG
            return 1, orig_token, False, None, None
        logging.debug("pathToStr() returned: "+file_path)
        logging.error("HERE5 remote_root="+remote_root) # DEBUG
        if file_path=="" or not file_path.startswith(remote_root): # file was moved out of our remote
            logging.error("HERE6") # DEBUG
            continue

        file_path=remove_root_and_decrypt_pathname_as_needed(file_path,remote_root,path_remote,remote_type_crypt)
        if file_path==None:
            continue

        if USE_SIMPLE_FILTER_MATCHING and not is_pathname_accepted_by_filter_file(file_path,filter_file):
            logging.debug("Filtered out by simple filtered matching: "+file_path)
            continue

        logging.debug("Change entry to merge: "+file_path)
        logging.error("RRR MERGING "+file_path)
        logging.error("RRRR ACTION="+file_action)

        # TODO This entire block is for applying the rclone filters. It is quite time-consuming for large sets of files. Consider applying the filters directly here instead of asking rclone to do it
        if file_action=="remove":
            file_path_adj=file_path+"/" if file_is_folder else file_path
            if file_path_adj in new_data:
                del new_data[file_path_adj]
                changes_made=True
                logging.debug("file deleted: "+file_path_adj)
                logging.error("RRRR file deleted: "+file_path_adj)
        else:
            added_dirs, added_files, changes_made, new_data=update_newdate_or_caches(COMBINED_LSL,file_id,file_path,file_is_folder,path,ofile,filter_file,options,added_dirs,added_files,changes_made,new_data)
            if changes_made==None:
                return 1, orig_token, False, None, None

    if COMBINED_LSL:
        for d in added_files.keys():
            logging.error("RR FILTER FOR depth "+str(d))
            changes_made2,new_data=fetch_file_info(path,new_data,[ f for (_,f) in added_files[d]],False,d,ofile,filter_file,options)
            if changes_made2==None:
                return 1, orig_token, False, None, None
            changes_made=changes_made or changes_made2
            # add the file IDs
            for (i,f) in added_files[d]:
                if f in new_data: # it may have been filtered out by fetch_file_info()
                    new_data[f]['id']=i
        for d in added_dirs.keys():
            logging.error("RR FILTER FOR depth "+str(d))
            changes_made2,new_data=fetch_file_info(path,new_data,[ f for (_,f) in added_dirs[d]],True,d,ofile,filter_file,options)
            if changes_made2==None:
                return 1, orig_token, False, None, None
            changes_made=changes_made or changes_made2
            # add the file IDs
            for (i,f) in added_dirs[d]:
                if f in new_data: # it may have been filtered out by fetch_file_info()
                    new_data[f]['id']=i

    if changes_made:
        logging.debug("writing changes to "+ofile)
        logging.error("RRRR writing changes to "+ofile)
        new_data=order_and_write_lsl_and_id(new_data,ofile) # store sorted dictionary
    
    return 0, token, changes_made, cached_data, new_data

def merge_gdrive_changes(token, path, lsl_file_cached, ofile, filter_file=None, options=None):
    return(merge_gdrive_changes_python(token, path, lsl_file_cached, ofile, filter_file, options))


def rclone_cmd(cmd, p1=None, p2=None, filter_file=None, files_file=None, options=None):
    """
    Execute an rclone command.
    p1 and p2 are optional.
    filter_file is a string full path to a file which will be passed to rclone --filter-from.
    files_file is a string full path to a list of files for the rclone command operation, such as copy these files.
    options is a list of switches passed to rclone
        EG: ["--filter-from", "some_file", "--dry-run", "-vv", '--log-format', '""']
    """
    linenum = inspect.getframeinfo(inspect.stack()[1][0]).lineno
    process_args = [rclone, cmd, "--config", rcconfig]
    if p1 is not None:
        process_args.append(p1)
    if p2 is not None:
        process_args.append(p2)
    if filter_file is not None:
        process_args.extend(["--filter-from", filter_file])
    if files_file is not None:
        process_args.extend(["--files-from-raw", files_file])
    if options is not None:
        process_args.extend(options)
    if args.rclone_args is not None:
        process_args.extend(args.rclone_args)
    logging.debug("    rclone command:  {}".format(process_args))
    logging.error("REMOVE ME!!!    rclone command:  {}".format(process_args))
    for x in range(MAXTRIES):
        try:
            p = subprocess.Popen(process_args)
            p.wait()
            if p.returncode == 0:
                return 0
        except Exception as e:
            logging.info(print_msg("WARNING", "rclone {} try {} failed.".format(cmd, x+1), p1))
            logging.info("message:  <{}>".format(e))
    logging.error(print_msg("ERROR", "rclone {} failed.  (Line {})".format(cmd, linenum), p1))
    return 1

def rclone_cmd_read_output(cmd, ofile, p1=None, p2=None, filter_file=None, files_file=None, options=None):
    """
    Execute an rclone command and save the output to ofile.
    p1 and p2 are optional.
    filter_file is a string full path to a file which will be passed to rclone --filter-from.
    files_file is a string full path to a list of files for the rclone command operation, such as copy these files.
    options is a list of switches passed to rclone
        EG: ["--filter-from", "some_file", "--dry-run", "-vv", '--log-format', '""']
    """
    linenum = inspect.getframeinfo(inspect.stack()[1][0]).lineno
    process_args = [rclone, cmd, "--config", rcconfig]
    if p1 is not None:
        process_args.append(p1)
    if p2 is not None:
        process_args.append(p2)
    if filter_file is not None:
        process_args.extend(["--filter-from", filter_file])
    if files_file is not None:
        process_args.extend(["--files-from-raw", files_file])
    if options is not None:
        process_args.extend(options)
    if args.rclone_args is not None:
        process_args.extend(args.rclone_args)
    logging.debug("    rclone command:  {}".format(process_args))
    for x in range(MAXTRIES):
        try:
            with io.open(ofile, "wt", encoding='utf8') as of:
                if not subprocess.call(process_args, stdout=of):
                    return 0
                logging.info(print_msg("WARNING", "rclone {} try {} failed.".format(cmd, x+1),p1))
        except Exception as e:
            logging.info(print_msg("WARNING", "rclone {} try {} failed.".format(cmd, x+1), p1))
            logging.info("message:  <{}>".format(e))
    logging.error(print_msg("ERROR", "rclone {} failed.  (Line {})".format(cmd, linenum), p1))
    return 1


LINE_FORMAT = re.compile(r'\s*([0-9]+) ([\d\-]+) ([\d:]+).([\d]+) (.*)')
def load_list(lslfile, filter_file=None):
    """
    Load the content of the lslfile into a dictionary.
    The key is the path to the file relative to the Path1/Path2 base.
    File size of -1, as for Google Docs files, prints a warning and are not loaded.

    lsl file format example:
          size <----- datetime (epoch) ----> key
       3009805 2013-09-16 04:13:50.000000000 12 - Wait.mp3
        541087 2017-06-19 21:23:28.610000000 DSC02478.JPG

    Returned sorted dictionary structure:
        OrderedDict([('RCLONE_TEST', {'size': '110', 'datetime': 946710000.0}),
                     ('file1.txt', {'size': '0', 'datetime': 946710000.0}), ...
    """
    d = {}
    try:
        line = "<none>"
        line_cnt = 0
        pp=re.compile(r'(\d{4})-(\d{2})-(\d{2}) (\d{2}):(\d{2}):(\d{2})')
        with io.open(lslfile, mode='rt', encoding='utf8') as f:
            for line in f:
                line_cnt += 1
                out = LINE_FORMAT.match(line)
                if out:
                    size = out.group(1)
                    date = out.group(2)
                    _time = out.group(3)
                    microsec = out.group(4)
                    if DISREGARD_MICROSEC:
                        # Android MediaStore's DATE_MODIFIED only returns the epoch in seconds
                        microsec="0"
                    # Using strptime() is VERY SLOW if the Android app is not in the foreground
                    # The following is a workaround
                    # https://stackoverflow.com/questions/14163399/convert-list-of-datestrings-to-datetime-very-slow-with-python-strptime
                    dt = datetime(*map(int, pp.match(date + ' ' + _time).groups()))
                    # And mktime() is also VERY SLOW under the same conditions
                    # timestamp() is faster and does not throw exceptions
                    date_time = dt.timestamp() + float('.'+ microsec)
#                    # on Android, time.mktime gives error for the hour of the Spring time change, e.g. 03/10/2019 at 2am (any time during 2am)
#                    # That's because, tecnically, that hour does not exist. So, if we get an error, we try the next hour, e.g. 3am
#                    # If it still gives error, then we fail.
#                    #date_time = time.mktime(datetime.strptime(date + ' ' + _time, '%Y-%m-%d %H:%M:%S').timetuple()) + float('.'+ microsec)
#                    dt = datetime.strptime(date + ' ' + _time, '%Y-%m-%d %H:%M:%S')
#                    try:
#                        date_time = time.mktime(dt.timetuple()) + float('.'+ microsec)
#                    except OverflowError:
#                        date_time = time.mktime((dt+timedelta(hours=1)).timetuple()) + float('.'+ microsec)
                    filename = out.group(5)
                    if USE_SIMPLE_FILTER_MATCHING and filter_file!=None and not is_pathname_accepted_by_filter_file(filename,filter_file):
                        logging.debug("load_list(): filtered out by simple filtered matching: "+filename)
                        continue
                    if filename in d:
                        logging.warning (f"WARNING  Duplicate line in LSL file:   <{line[:-1]}>")
                        strtime = datetime.fromtimestamp(d[filename]['datetime']).strftime("%Y-%m-%d %H:%M:%S.%f")
                        logging.warning (f"         Prior found (keeping latest): <{d[filename]['size']:>9} {strtime}    {filename}>")
                        if date_time > d[filename]['datetime']:
                            d[filename] = {'size': size, 'datetime': date_time, 'orig_datetime': date+" "+_time+"."+microsec}
                    else:
                        d[filename] = {'size': size, 'datetime': date_time, 'orig_datetime': date+" "+_time+"."+microsec}

                else:
                    logging.warning("Something wrong with this line (ignored) in {}.  (Google Doc files cannot be synced.):\n   <{}>".format(lslfile, line))
        return 0, collections.OrderedDict(sorted(d.items()))        # return Success and a sorted list

    except Exception as e:
        logging.error(f"Exception in load_list loading <{lslfile}>:\n  <{e}>\n  Line # {line_cnt}:  {line}")
        return 1, ""                                                # return False

def override_filename(lsl_file):
    return(lsl_file+"_OVERRIDE")

def override_filename_exists(lsl_file):
    return(os.path.exists(override_filename(lsl_file)))

# Only used for local filesystem (Path2)
def cached_token_filename(lsl_file):
    return(lsl_file+"_TOKEN")

# Only used for local filesystem (Path2)
def new_token_filename(lsl_file):
    return(lsl_file+"_TOKEN_NEW")

def dump_path_listing(path12, lsl_file, filter_file, extra_filter_file=None):
    if rclone_lsl(path12, lsl_file, filter_file=user_filter_file, extra_filter_file=extra_filter_file):
        logging.error("rclone lsl failed for "+str(path12)+" with filter from "+str(user_filter_file))
        return True
    if rclone_lsd(path12, lsl_file, filter_file=user_filter_file, extra_filter_file=extra_filter_file, append=True):
        logging.error("rclone lsd failed for "+srt(path12)+" with filter from "+str(user_filter_file))
        return True
    return False

def get_and_load_lsl(path_text, lsl_file, path12=None):
    """
    Optionally call for an rclone lsl of the referenced path12, written to the lsl_file.
    Then load the lsl file content into into a dictionary and return the dictionary to the caller.

    path_text is a convenience string for logging, eg "Path2 prior"
    lsl_file is the full path string for file to be written to by the rclone lsl, and read from for loading the content.
    path12 is a string to be passed to the rclone lsl, eg "Dropbox:"
    """
    if path12 is not None:
        if override_filename_exists(lsl_file):
            logging.info(print_msg("INFO", "Using lsl file from app: "+override_filename(lsl_file)))
            logging.error(print_msg("RRR INFO", "Using lsl file from app: "+override_filename(lsl_file)))
            shutil.copy(override_filename(lsl_file),override_filename(lsl_file)+"_BAK")
            shutil.move(override_filename(lsl_file),lsl_file)
        else:
#            if rclone_lsl(path12, lsl_file, filter_file=user_filter_file):
#                return RTN_ABORT, None
#            if rclone_lsd(path12, lsl_file, filter_file=user_filter_file, append=True):
#                return RTN_ABORT, None
            if dump_path_listing(path12, lsl_file, user_filter_file):
                return RTN_ABORT, None
    status, loaded_list = load_list(lsl_file, user_filter_file)
    if status:
        logging.error(print_msg("ERROR", f"Failed loading {path_text} list file <{lsl_file}>"))
        return RTN_ABORT, None
    if not first_sync and len(loaded_list) == 0:
        logging.error(print_msg("ERROR", f"Zero length in {path_text} list file <{lsl_file}>.  Cannot sync to an empty directory tree."))
        return RTN_CRITICAL, None
    return 0, loaded_list

def incremental_get_and_load_lsl(path_text, change_token, lsl_file_cached, lsl_file, path12=None):
    """
    Optionally call for an rclone lsl of the referenced path12, written to the lsl_file.
    Then load the lsl file content into into a dictionary and return the dictionary to the caller.

    path_text is a convenience string for logging, eg "Path2 prior"
    lsl_file is the full path string for file to be written to by the rclone lsl, and read from for loading the content.
    path12 is a string to be passed to the rclone lsl, eg "Dropbox:"
    """
    if path12 is None:
        logging.error("incremental_get_and_load_lsls() called without path12")
        return RTN_CRITICAL, None, None, False, change_token

    loaded_list = None
    status, change_token, changes_detected, cached_data, loaded_list = merge_gdrive_changes(change_token, path12, lsl_file_cached, lsl_file, filter_file=user_filter_file)
    logging.debug("incremental_get_and_load_lsls(): merging complete")
    if status != 0:
        return RTN_ABORT, None, None, False, change_token

    if changes_detected:
        if loaded_list==None:
            status, loaded_list = load_list(lsl_file, user_filter_file)
            if status:
                logging.error(print_msg("ERROR", f"Failed loading {path_text} list file <{lsl_file}>"))
                return RTN_ABORT, None, None, False, change_token
            (_,loaded_list)=merge_id_list(loaded_list,lsl_file)
        if not first_sync and len(loaded_list) == 0:
            logging.error(print_msg("ERROR", f"Zero length in {path_text} list file <{lsl_file}>.  Cannot sync to an empty directory tree."))
            return RTN_CRITICAL, None, None, False, change_token
    return 0, loaded_list, cached_data, changes_detected, change_token

def request_lock(caller, lock_file):
    for _ in range(5):
        if os.path.exists(lock_file):
            with io.open(lock_file, mode='rt', encoding='utf8',errors="replace") as fd:
                locked_by = fd.read()
                logging.info("Lock file exists - Waiting a sec: <{}>\n<{}>".format(lock_file, locked_by[:-1]))   # remove the \n
            time.sleep(1)
        else:  
            with io.open(lock_file, mode='wt', encoding='utf8') as fd:
                fd.write("Locked by {} at {}\n".format(caller, time.asctime(time.localtime())))
                logging.info("Lock file created: <{}>".format(lock_file))
            return 0
    logging.warning("Timed out waiting for lock file to be cleared: <{}>".format(lock_file))
    return -1

def release_lock(lock_file):
    if os.path.exists(lock_file):
        logging.info("Lock file removed: <{}>".format(lock_file))
        os.remove(lock_file)
        return 0
    else:
        logging.warning("Attempted to remove lock file but the file does not exist: <{}>".format(lock_file))
        return -1

def keyboardInterruptHandler(signal, frame):
    logging.error("***** KeyboardInterrupt Critical Error Abort - Must run --first-sync to recover.  See README.md *****\n")
    if os.path.exists(path2_lsl_file):
        shutil.move(path2_lsl_file, path2_lsl_file + '_ERROR')
    if os.path.exists(path1_lsl_file):
        shutil.move(path1_lsl_file, path1_lsl_file + '_ERROR')
    release_lock(lock_file)
    sys.exit(2)
signal.signal(signal.SIGINT, keyboardInterruptHandler)
    

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="***** BiDirectional Sync for Cloud Services using rclone *****")
    parser.add_argument('Path1',
                        help="Local path, or cloud service with ':' plus optional path.  Type 'rclone listremotes' for list of configured remotes.")
    parser.add_argument('Path2',
                        help="Local path, or cloud service with ':' plus optional path.  Type 'rclone listremotes' for list of configured remotes.")
    parser.add_argument('-1', '--first-sync', action='store_true',
                        help="First run setup.  WARNING: Path1 files may overwrite path2 versions.  Consider using with --dry-run first.  Also asserts --verbose.")
    parser.add_argument('-c', '--check-access', action='store_true',
                        help="Ensure expected RCLONE_TEST files are found on both path1 and path2 filesystems, else abort.")
    parser.add_argument('--check-filename', default=CHK_FILE, 
                        help=f"Filename for --check-access (default is <{CHK_FILE}>).")
    parser.add_argument('-D', '--max-deletes', type=int, default=MAX_DELETE,
                        help=f"Safety check for percent maximum deletes allowed (default {MAX_DELETE}%%).  If exceeded the rclonesync run will abort.  See --force.")
    parser.add_argument('-F', '--force', action='store_true',
                        help="Bypass --max-deletes safety check and run the sync.  Also asserts --verbose.")
    parser.add_argument('--no-check-sync', action='store_true',
                        help="Disable comparison of final LSL files (default is check-sync enabled).")
    parser.add_argument('--check-sync-only', action='store_true',
                        help="Only execute the comparison of LSL files from the last rclonesync run.")
# no longer needed
#    parser.add_argument('-e', '--remove-empty-directories', action='store_true',
#                        help="Execute rclone rmdirs as a final cleanup step.")
    parser.add_argument('-f','--filters-file', default=None,
                        help="File containing rclone file/path filters (needed for Dropbox).")
    parser.add_argument('-r','--rclone', default="rclone",
                        help="Path to rclone executable (default is rclone in path environment var).")
    parser.add_argument('--config', default=None,
                        help="Path to rclone config file (default is typically ~/.config/rclone/rclone.conf).")
    parser.add_argument('--rclone-args', nargs=argparse.REMAINDER,
                        help="Optional argument(s) to be passed to rclone.  Specify this switch and rclone ags at the end of rclonesync command line.")
    parser.add_argument('-v', '--verbose', action='count', default=0,
                        help="Enable event logging with per-file details.  Specify once for info and twice for debug detail.")
    parser.add_argument('--rc-verbose', action='count',
                        help="Enable rclone's verbosity levels (May be specified more than once for more details.  Also asserts --verbose.)")
    parser.add_argument('-d', '--dry-run', action='store_true',
                        help="Go thru the motions - No files are copied/deleted.  Also asserts --verbose.")
    parser.add_argument('-i', '--create-id-list', action='store_true',
                        help="Force the creation of the id list. For older installations. Normally the id list is created during first-sync.")
    parser.add_argument('-w', '--workdir', default=os.path.expanduser("~/.rclonesyncwd"),
                        help="Specified working dir - useful for testing.  Default is ~user/.rclonesyncwd.")
    parser.add_argument('--no-datetime-log', action='store_true',
                        help="Disable date-time from log output - useful for testing.")
    parser.add_argument('--no-cleanup', action='store_true',
                        help="Retain working files - useful for debug and testing.")
    parser.add_argument('--path1-changes-only', action='store_true',
                        help="Only check for changes in path1 and return.")
    parser.add_argument('-V', '--version', action='version', version='%(prog)s ' + __version__,
                        help="Return rclonesync's version number and exit.")
    args = parser.parse_args()
    
    first_sync   =  args.first_sync
    check_access =  args.check_access
    chk_file     =  args.check_filename
    max_deletes  =  args.max_deletes
    verbose      =  args.verbose
    rc_verbose   =  args.rc_verbose
    if rc_verbose == None: rc_verbose = 0
    user_filter_file =  args.filters_file
    rclone       =  args.rclone
    dry_run      =  args.dry_run
    do_create_id_list      =  args.create_id_list
    force        =  args.force
    rmdirs       =  False #args.remove_empty_directories

    # Set up logging
    if not args.no_datetime_log:
        logging.basicConfig(format='%(asctime)s:  %(message)s') # /%(levelname)s/%(module)s/%(funcName)s
    else:
        logging.basicConfig(format='%(message)s')

    if verbose >= 2:
        logging.getLogger().setLevel(logging.DEBUG)             # Log debug detail
    elif verbose>0 or rc_verbose>0: # or force or first_sync or dry_run:
        logging.getLogger().setLevel(logging.INFO)              # Log each file transaction
    else:
        logging.getLogger().setLevel(logging.WARNING)           # Log only unusual events.  Normally silent.

    logging.info(f"***** BiDirectional Sync for Cloud Services using rclone ({__version__}) *****")


    # Environment checks
    if is_Windows:
        chcp = subprocess.check_output(["chcp"], shell=True).decode("utf-8")
        err = False
        py_encode_env = ''
        if "PYTHONIOENCODING" in os.environ:
            py_encode_env = os.environ["PYTHONIOENCODING"]
        if "65001" not in chcp:
            logging.error ("ERROR  In the Windows CMD shell execute <chcp 65001> to enable support for UTF-8.")
            err = True
        if py_encode_env.lower().replace('-','') != "utf8":
            logging.error ("ERROR  In the Windows CMD shell execute <set PYTHONIOENCODING=UTF-8> to enable support for UTF-8.")
            err = True
        if err:
            sys.exit(1)


    # Check workdir goodness
    workdir = args.workdir
    if not (workdir.endswith('/') or workdir.endswith('\\')):   # 2nd check is for Windows paths
        workdir += '/'
    try:
        if not os.path.exists(workdir):
            os.makedirs(workdir)
    except Exception as e:
        logging.error(f"ERROR  Cannot access workdir at <{workdir}>.")
        sys.exit(1)


    # Check rclone related goodness
    rcversion_FORMAT = re.compile(r"v?(\d+)\.(\d+).*")
    try:
        rclone_V = subprocess.check_output([rclone, "version"]).decode("utf8").split()[1]
    except Exception as e:
        logging.error(f"ERROR  Cannot invoke <rclone version>.  rclone not installed or invalid --rclone path?\nError message: {e}.\n")
        sys.exit(1)
    out = rcversion_FORMAT.match(rclone_V)
    if out:
        rcversion = int(out.group(1)) + float("0." + out.group(2))
        if rcversion < RCLONE_MIN_VERSION:
            logging.error(f"ERROR  rclone minimum version is v{RCLONE_MIN_VERSION}.  Found version v{rcversion}.")
            sys.exit(1)
    else:
        logging.error(f"ERROR  Cannot get rclone version info.  Check rclone installation - minimum version v{RCLONE_MIN_VERSION}.")
        sys.exit(1)

    rcconfig = args.config
    if rcconfig is None:
        try:  # Extract the second line from the two line <rclone config file> output similar to:
                # Configuration file is stored at:
                # /home/<me>/.config/rclone/rclone.conf
            rcconfig = str(subprocess.check_output([rclone, "config", "file"]).decode("utf8")).split(':\n')[1].strip()
        except Exception as e:
            logging.error(f"ERROR  Cannot get <rclone config file> path.  rclone install problem?\nError message: {e}.\n")
            sys.exit(1)
    if not os.path.exists(rcconfig):
        logging.error(f"ERROR  rclone config file <{rcconfig}> not found.  Check rclone configuration, or invalid --config switch?")
        sys.exit(1)

    try:
        clouds = subprocess.check_output([rclone, "listremotes", "--config", rcconfig]).decode("utf8") .split("\n")[:-1]
    except Exception as e:
        logging.error("ERROR  Cannot get list of known remotes.  Have you run rclone config?")
        sys.exit(1)


    # Set up Path1 / Path2
    def pathparse(path):
        """Handle variations in a path argument.
        Cloud:              - Root of the defined cloud
        Cloud:some/path     - Supported with our without path leading '/'s
        X:                  - Windows drive letter
        X:\\some\\path      - Windows drive letter with absolute or relative path
        some/path           - Relative path from cwd (and on current drive on Windows)
        //server/path       - UNC paths are supported
        On Windows a one-character cloud name is not supported - it will be interprested as a drive letter.
        """
        _cloud = False
        path_base = ''
        if ':' in path:
            if len(path) == 1:                                  # Handle corner case of ':' only passed in
                logging.error("ERROR  Path argument <{}> not a legal path.".format(path))
                sys.exit(1)
            if path[1] == ':' and is_Windows:                   # Windows drive letter case
                path_base = path
                if not path_base.endswith('\\'):                # For consistency ensure the path ends with '/'
                    path_base += '/'
            else:                                               # Cloud case with optional path part
                # path_FORMAT = re.compile(r'([\w-]+):(.*)')
                path_FORMAT = re.compile(r'([ \w-]+):(.*)')
                out = path_FORMAT.match(path)
                if out:
                    _cloud = True
                    cloud_name = out.group(1) + ':'
                    if cloud_name not in clouds:
                        logging.error(f"ERROR  Path argument <{cloud_name}> not in list of configured Clouds: {clouds}.")
                        sys.exit(1)
                    path_part = out.group(2)
                    if path_part:
                        if not (path_part.endswith('/') or path_part.endswith('\\')):    # 2nd check is for Windows paths
                            path_part += '/'
                    path_base = cloud_name + path_part
        else:                                                   # Local path (without Windows drive letter)
            path_base = path
            if not (path_base.endswith('/') or path_base.endswith('\\')):
                path_base += '/'

        if not _cloud:
            if not os.path.exists(path_base):
                logging.error(f"ERROR  Local path parameter <{path_base}> cannot be accessed.  Path error?")
                sys.exit(1)

        return path_base

    path1_base = pathparse(args.Path1)
    path2_base = pathparse(args.Path2)

    # Run the job
    lock_file = os.path.join(tempfile.gettempdir(), 'rclonesync_LOCK_' + (
        path1_base + path2_base).replace(':','_').replace(r'/','_').replace('\\','_'))

    if args.path1_changes_only or request_lock(sys.argv, lock_file) == 0:
        status = bidirSync()
        if not args.path1_changes_only: release_lock(lock_file)
        if status == RTN_CRITICAL:
            logging.error("***** Critical Error Abort - Must run --first-sync to recover.  See README.md *****\n")
            if os.path.exists(path2_lsl_file):
                shutil.move(path2_lsl_file, path2_lsl_file + '_ERROR')
            if os.path.exists(path1_lsl_file):
                shutil.move(path1_lsl_file, path1_lsl_file + '_ERROR')
            sys.exit(2)
        if status == RTN_ABORT:
            logging.error("***** Error Abort.  Try running rclonesync again. *****\n")
            sys.exit(1)
        if status == 0:
            logging.info(">>>>> Successful run.  All done.\n")
            sys.exit(0)
    else:
        logging.warning("***** Prior lock file in place, aborting.  Try running rclonesync again. *****\n")
        sys.exit(1)
